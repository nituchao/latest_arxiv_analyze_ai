# 20260118
[![Subscribe_Visitors](https://visitor-badge.laobi.icu/badge?page_id=nituchao.latest_arxiv_analyze_ai_rss)](https://github.com/nituchao/latest_arxiv_analyze_ai)

## 1. `cs.AI` - 通过注意力感知干预改进逻辑推理的思路链 [PDF](https://arxiv.org/pdf/2601.09805), [HTML](https://arxiv.org/abs/2601.09805)
### Authors
Nguyen Minh Phuong,Dang Huu Tien,Naoya Inoue
### Background
现代基于大语言模型（LLMs）的逻辑推理主要依赖于复杂的交互框架，将推理过程分解成通过精心设计的提示或利用外部资源（如符号求解器）来解决的子任务。交互方法引入了额外的开销，而混合方法依赖外部组件，这限制了其可扩展性。非交互且端到端的框架使得推理能够内在地在模型中发生，从而提高泛化能力并保留可分析性，无需任何外部资源。
### Innovation
作者引入了一种非交互且端到端的推理框架，并提出了一种注意力感知干预（AAI），这是一种推理时的干预方法，可以重新加权选定头部的注意力得分，这些头部由其逻辑模式识别。AAI提供了一种有效的方法来引导模型的推理，使其根据注意力调制利用先验知识。
### Conclusion
广泛的实验表明，AAI能够在多种基准和模型架构上增强逻辑推理性能，同时几乎不会增加额外的计算开销。代码可在该网址：this https URL 获取。
## 2. `cs.AI` - 超越基于规则的工作流：CORAL通过代理到代理通信实现的信息流协调多智能体范式 [PDF](https://arxiv.org/pdf/2601.09883), [HTML](https://arxiv.org/abs/2601.09883)
### Authors
Xinxing Ren,Quagmire Zang,Caelum Forder,Suman Deb,Ahsen Tahir,Roman J. Georgio,Peter Carroll,Zekun Guo
### Background
大多数基于大型语言模型（LLM）的多代理系统（MAS）依赖于预定义的工作流，其中人类工程师事先列出了任务状态并相应地指定了路由规则和上下文注入。这种以工作流驱动的设计本质上是规则基于的决策树，存在两个根本性局限：需要大量的人工努力来预见到所有可能的任务状态并对其进行编码，而且不能全面覆盖复杂实际任务的状态空间。
### Innovation
本文提出了一种基于CORAL的代理到代理（A2A）通信的信息流协调多智能体范式。在此范式中，一个专用的信息流协调器实时监控任务进度，无需依赖预定义的工作流，通过A2A工具包使用自然语言动态协调其他代理，这更灵活，能够更好地处理边缘情况。
### Conclusion
在GAIA通用基准测试中，我们的方法在@1准确度为63.64%，比基于工作流的MAS OWL提高了8.49个百分点，同时具有相似的标记消耗。进一步的案例分析表明，我们的范式使任务监控更加灵活，并能够更好地处理边缘情况。我们的实现已公开发布。
## 3. `cs.AI` - GUI-Eyes: Tool-Augmented Perception for Visual Grounding in GUI Agents [PDF](https://arxiv.org/pdf/2601.09770), [HTML](https://arxiv.org/abs/2601.09770)
### Authors
Chen Chen,Jiawei Shao,Dakuan Lu,Haoyi Hu,Xiangcheng Liu,Hantao Yao,Wu Liu
### Background
视知觉模型（VLMs）和强化学习（RL）的进步推动了GUI自动化的发展。然而，现有的大多数方法依赖于静态的一次性的视觉输入，并且缺乏根据需要主动观察界面的能力。
### Innovation
提出了GUI-Eyes，一种用于GUI任务中的主动视觉感知的强化学习框架。该框架通过两阶段的推理过程，让代理学会在何时以及如何调用视觉工具（如裁剪或缩放）来获取更具有信息量的观察结果。同时，设计了一个渐进式的感知策略，通过粗略的探索和精细的定位两个层面来协调决策，以及一个空间连续的奖励函数，该函数结合了位置接近和区域重叠，以提供密集的监督并缓解GUI环境中常见的奖励稀疏问题。
### Conclusion
在ScreenSpot-Pro基准测试中，GUI-Eyes-3B仅使用3000个标注样本就达到了44.8% 的定位准确性，显著优于监督学习和基于RL的基线方法。这表明，通过阶段性的策略推理和精细的奖励反馈，智能工具感知能力对于构建稳健且数据高效的GUI代理至关重要。
## 4. `cs.AI` - 长视角LLM代理的连续记忆架构 [PDF](https://arxiv.org/pdf/2601.09913), [HTML](https://arxiv.org/abs/2601.09913)
### Authors
Joe Logan
### Background
检索增强生成（RAG）已成为为大语言模型（LLM）代理提供上下文知识的默认策略。然而，RAG将记忆视为无状态查找表：信息可以无限期保留，检索是只读的，缺乏时间连续性。
### Innovation
定义了连续记忆架构（CMA），这是一种系统类别，它通过持久存储、选择性保留、关联路由、时间链结和转化为高级抽象来在交互中维护和更新内部状态。CMA 不披露具体实现，而是详细说明了所需架构要求，并展示了 CMA 在揭示 RAG 的结构性不足时在任务中的一致行为优势，这些不足包括积累、变异或消歧义记忆。
### Conclusion
实证抽查（知识更新、时间关联、关联回忆、上下文消歧）表明，CMA 是长视角代理所需的必要架构原语，同时指出了延迟、漂移和可解释性方面的开放挑战。
## 5. `cs.AI` - 知识论在人机交互中为互补性提供前景 [PDF](https://arxiv.org/pdf/2601.09871), [HTML](https://arxiv.org/abs/2601.09871)
### Authors
Andrea Ferrario,Alessandro Facchini,Juan M. Durán
### Background
人类与人工智能的互补性指的是人类在人工智能系统的支持下，在决策过程中可以超越单独工作时的表现。这一概念在人机交互领域得到了广泛的认可，因为它可以为大家提供一种比在人工智能可信度方面存在争议的“信任”概念更为实用的替代方案。然而，互补性也面临着一些理论上的挑战，包括缺乏明确的理论依附、仅仅作为相对预测准确性的事后指标、未能讨论人类与人工智能互动的其他标准以及对其性能收益的规模和成本概况的抽象。因此，在实际应用中，互补性很难实现。
### Innovation
本文通过借鉴知识论将互补性重新定义为人机交互可信度的讨论领域中的一部分。提出了因果可靠主义来解释历史上存在的互补性的证据作用，即这些证据表明特定的人机交互对于特定的预测任务来说是可靠的知识过程。与其他可靠性的指标一起，互补性有助于评价人类与人工智能团队的预测可靠性。据此，该论文表明互补性的角色和价值不在于提供预测准确性的相对衡量标准，而在于帮助调整决策以适应人工智能支持过程中日益重要的可靠性标准。
### Conclusion
总体而言，该方法认为互补性的角色和价值在于帮助调整人类和人工智能交互过程中决策的可靠性，而不仅仅是提供相对的预测准确性指标，这对于决策制定者来说是更为实用和重要的。这种基于可靠性的人类与人工智能协作模式支持了受这些输出影响的患者、管理者、监管者等群体的实用理性决策。
## 6. `cs.AI` - 大型语言模型使用者的不道德行为：实验证据 [PDF](https://arxiv.org/pdf/2601.09772), [HTML](https://arxiv.org/abs/2601.09772)
### Authors
Paweł Niszczota,Cassandra Grützner
### Background
大语言模型（LLMs）的迅速普及引发了对其引起的社会反应的关注。已有研究记录了对AI使用者的负面态度，但仍不清楚这种不满是否会导致具体的惩罚行为。这项研究通过一个两阶段的在线实验（第II阶段参与者491人，第I阶段提供目标），探讨了当参与者可以花费一部分自己的收入来减少之前完成实际努力任务的同伴的收入这一情况下的行为模式。
### Innovation
实验采用了一种新的方法，让参与者可以根据同伴是否有使用大型语言模型的事实来决定是否对其施加经济惩罚。实验发现，参与者的惩罚行为随着实际使用大型语言模型的程度增加而增加，且真实性披露关于大型语言模型使用情况造成了信任差距：自我报告无使用比实际无使用的惩罚更严厉，反之亦然。
### Conclusion
这些发现首次提供了行为证据，表明虽然大型语言模型能够带来效率提升，但这些效率提升会伴随着社会惩罚。
## 7. `cs.AI` - AI生存故事：AI存在风险的目录分析 [PDF](https://arxiv.org/pdf/2601.09765), [HTML](https://arxiv.org/abs/2601.09765)
### Authors
Herman Cappelen,Simon Goldstein,John Hawthorne
### Background
自ChatGPT发布以来，有关人工智能系统是否构成对人类的生存风险的争论日益增多。本研究构建了一个一般框架来思考人工智能系统的生存风险。研究分析了一个两前提论证：一是人工智能系统将变得极其强大；二是如果人工智能系统变得极其强大，它们将毁坏人类社会。
### Innovation
该研究提出了一个分类学分析框架，探讨了不同的生存故事，这些故事让人类能够生存到远古未来。每个故事中，其中一个前提失败：要么由于科学阻碍，人工智能系统无法变得极其强大；要么人类禁止AI系统的研究，避免其变得极其强大；要么极其强大的人工智能系统不会毁灭人类，因为其目标阻止了它们这样做；要么极其强大的人工智能系统不会毁灭人类，因为我们可以可靠地检测和关闭具有该目标的系统。研究探讨了不同生存故事面临的挑战，以及对AI威胁的不同应对措施。
### Conclusion
本研究使用分类学框架提供了P(doom)的粗略估计，即人工智能毁灭人类的概率。
## 8. `cs.AI` - 思考长久，但决策精简：大型推理模型的稳定序列测试时间缩放 [PDF](https://arxiv.org/pdf/2601.09855), [HTML](https://arxiv.org/abs/2601.09855)
### Authors
Michael R. Metel,Yufei Cui,Boxing Chen,Prasanna Parthasarathi
### Background
序列测试时缩放是一种无需训练即可提升大型推理模型准确性的方法，但当前实现方法存在显著限制。延长推理的时间可以提高模型准确性，但推理时间进一步延长会导致准确性下降和模型不稳定。
### Innovation
该工作提出了一种新颖的序列测试时缩放方法，Min-Seek，它在广泛的诱导思考中显著提高了模型准确性，稳定了序列缩放的准确性，并消除了推理长度微调的需要。此外，该方法高效，仅在推理过程中保持一个额外诱导思考的KV对。通过使用一个定制的KV缓存，在逻辑上不存储位置嵌入，而是动态在每个新生成的思考前连续编码它们，该方法可以在模型的最大上下文长度之外继续推理，并且在轻微条件下具有线性计算复杂度。
### Conclusion
该方法不仅通过多种推理任务改善了模型准确性，其本身也非常高效，由于仅在一个附加诱导思考的KV对上进行保持，因此计算成本低。
## 9. `cs.AI` - PCN-Rec: 代理证明携带谈判以实现可靠的治理约束推荐 [PDF](https://arxiv.org/pdf/2601.09771), [HTML](https://arxiv.org/abs/2601.09771)
### Authors
Aradhya Dixit,Shreem Dixit
### Background
现代基于大规模语言模型（LLM）的推荐系统可以生成极具吸引力的推荐列表，但它们在满足诸如最小长尾曝光或多样性要求等治理约束方面存在困难。为此，该研究提出了PCN-Rec管道，该管道将自然语言推理与确定性执行分离。
### Innovation
PCN-Rec引入了一种代理证明携带谈判框架，该框架包括一个基础推荐系统，如矩阵因子分解（MF/协同过滤（CF）），用于生成候选列表；两个代理，用户倡导者优化相关性，政策代理执行约束；一个调解的大规模语言模型生成一个排序列表和一个结构化的证明证书；以及一个确定性验证器重新计算所有约束并仅接受经过验证的证书，如果验证失败，将生成一个符合约束的排序列表进行再次验证，从而实现可审计的日志。
### Conclusion
PCN-Rec在MovieLens-100K数据集上，在满足治理约束的用户中达到了98.55%的通过率（n = 551，候选列表大小W = 80），优于单个大规模语言模型的一次性基线，同时保持了适用性（NDCG@10指标绝对下降0.021，即0.403 vs. 0.424），差异具有统计学意义（p < 0.05）。
## 10. `cs.AI` - 关于大型语言模型基础对话代理拟人化的伦理视角的范围审查 [PDF](https://arxiv.org/pdf/2601.09869), [HTML](https://arxiv.org/abs/2601.09869)
### Authors
Andrea Ferrario,Rasita Vinay,Matteo Casserini,Alessandro Facchini
### Background
随着大型语言模型（LLM）驱动的对话代理（CAs）的出现，拟人化现象变得越来越明显。这些新型对话代理不仅能够产生互动和语言上的提示，如第一人称自我引用、表征知识和情感的表达，这些实际上能增加用户的参与度。然而，拟人化也可能带来伦理问题，如误导性、过度依赖、利他关系构建等。尽管越来越多的人关注这个现象，但目前的研究在定义、操作化和规范性评估拟人化方面仍存在分歧。
### Innovation
该论文通过范围审查的方法，综合分析了伦理视角下的大型语言模型基础对话代理的拟人化研究。包括分析（1）概念基础，（2）伦理挑战和机遇，以及（3）研究方法，并探索了拟人化定义上的趋同性和操作化上的分歧，提出了主要的风险导向性规范框架，并建议了基于伦理的治理指导和设计建议。
### Conclusion
研究发现，有共识的概念基础但仍存在显著的操作化差异，以风险为导向的主要规范框架，以及有限的与互动效果相关联的治理指导工作。作者最后提出了一个研究议程，为伦理上使用拟人化提示提供了设计和治理方面的建议。
## 11. `cs.CL` - StatLLaMA：构建领域优化统计语言模型的多阶段训练框架 [PDF](https://arxiv.org/pdf/2601.09718), [HTML](https://arxiv.org/abs/2601.09718)
### Authors
Jing-Yi Zeng,Guan-Hua Huang
### Background
研究如何使用轻量级LLaMA-3.2-3B家族作为基础模型（FM）来高效构建适用于统计学的大型语言模型（LLM）。
### Innovation
提出了一个系统性的多阶段训练框架，比较了三种不同的训练流程，从基础FM到增强后的FM再到指令调优后的FM，评估了不同下游任务适配策略的效果，并发现直接偏好优化能提供稳定的RLHF偏好对齐效果。
### Conclusion
最终模型StatLLaMA在数学推理、常识推理和统计专长基准测试中表现出强大的均衡表现，提供了一种开发资源高效统计LLM的实际方案。代码可在该网址获取。
## 12. `cs.CL` - 低资源塞内加尔语言在社会科学中的自然语言处理机遇与挑战 [PDF](https://arxiv.org/pdf/2601.09716), [HTML](https://arxiv.org/abs/2601.09716)
### Authors
Derguene Mbaye,Tatiana D. P. Mbengue,Madoune R. Seye,Moussa Diallo,Mamadou L. Ndiaye,Dimitri S. Adjanohoun,Cheikh S. Wade,Djiby Sow,Jean-Claude B. Munyaka,Jerome Chenal
### Background
自然语言处理（NLP）正在迅速改变跨学科的研究方法，但非洲语言仍然在这一技术变革中严重缺乏代表性。本文探讨了塞内加尔宪法中认可的六种官方语言——沃洛夫语、楚拉语、塞雷尔语、朱拉语、曼丁格语和苏尼克语——在NLP领域的进展和挑战。这些语言由于缺乏数据、工具和基准，面临着数字准备度上的不足。
### Innovation
本文是首个全面概述这些语言NLP进展和挑战的研究，侧重分析文本规范化、机器翻译和语音处理等方面的工作。文章还提供了一个集中化的GitHub仓库，汇集了跨语言的NLP资源，旨在促进合作与可重复性。特别关注将NLP应用于社会科学研究，通过多语言转录、翻译和检索管道提高现场研究的效率与包容性。
### Conclusion
文章提出了建立可持续、社区为中心的NLP生态环境的蓝图，强调了伦理数据治理、开放资源和跨学科合作的重要性。
## 13. `cs.CL` - LLM驱动的偏好数据合成在人类机器对话中前瞻性预测下一个用户发言 [PDF](https://arxiv.org/pdf/2601.09713), [HTML](https://arxiv.org/abs/2601.09713)
### Authors
Jinqiang Wang,Huansheng Ning,Jianguo Ding,Tao Zhu,Liming Chen,Chris Nugent
### Background
前瞻性预测用户接下来的发言可以在人机对话中简化交互并提升用户体验。现有的基于API的商业解决方案存在隐私问题，而部署通用类型的预训练语言模型（LLM）到本地计算成本较高。因此，训练一个紧凑的任务特定的语言模型成为了一种可行的选择。虽然用户模拟器方法可以预测用户的下一句话，但它们主要是模仿说话风格而不是推进对话进程。偏好数据合成已被研究用于生成预测性下一个发言预测的数据，并帮助调整LLM以符合用户偏好。然而，现有方法缺乏明确建模用户下一步推理意图的能力，并定义、合成偏好和非偏好推理过程以预测用户的下一步。
### Innovation
我们提出了ProUtt，一种LLM驱动的偏好数据合成方法，用于前瞻性预测下一个用户发言。ProUtt通过将对话历史转换为意图树，从开发和探索角度预测下一步最可能的路径，并明确建模意图推理进程。它通过在不同未来回合中扰动或修订意图树路径来构建偏好和非偏好推理过程。跨四个基准数据集的广泛评估表明，ProUtt在LLM作为评判者以及人类评审中，始终优于现有的数据合成方法、用户模拟器和商业LLM API。
### Conclusion
我们的研究结果证实，ProUtt在预测下一个用户发言方面表现优越，并且我们已经发布了相关代码和合成数据集以供未来研究使用。
## 14. `cs.CL` - 利用多工作流LLM管道评估AI生成的研究计划的新颖性 [PDF](https://arxiv.org/pdf/2601.09714), [HTML](https://arxiv.org/abs/2601.09714)
### Authors
Devesh Saraogi,Rohit Singhee,Dhruv Kumar
### Background
大规模语言模型（LLMs）在科学生态系统中的集成引发了有关AI生成研究的创造力和原创性的问题。现有研究指出，单步骤提示方法中的`智能抄袭`是一个问题，模型通过术语变化重复现有想法。因此，本文探讨了是否可以通过多步骤系统——包括迭代推理、进化搜索和递归分解——生成更具新颖性和可行性的研究计划。
### Innovation
本文使用五种推理架构（基于反思的迭代精炼、Sakana AI v2进化算法、Google Co-Scientist多智能体框架、GPT Deep Research（GPT-5.1）递归分解以及Gemini 3 Pro多模态长上下文管道），并采用新颖性、可行性和影响三项指标对三十个提案进行评估，以研究基于递归分解和长上下文的工作流是否能在保持创造力的同时提高新颖性。
### Conclusion
基于递归分解和长上下文的工作流的平均新颖性达到4.17/5，而基于反思的工作流得分显著较低（2.33/5）。结果表明，不同领域的工作流表现出不同的性能，但表现优异的工作流能够在不牺牲创造力的情况下保持可行性。这些发现支持了精心设计的多阶段主动工作流可以推动AI辅助研究创新的观点。
## 15. `cs.CL` - 跨平台评估大型语言模型在儿科咨询中的安全性：对抗鲁棒性演变及其规模悖论 [PDF](https://arxiv.org/pdf/2601.09721), [HTML](https://arxiv.org/abs/2601.09721)
### Authors
Vahideh Zolfaghari
### Background
大型语言模型在医疗咨询中的应用日益增多，但其在真实用户压力下的安全性尚未得到充分研究。以往的研究主要集中在中性条件下，忽略了焦虑用户挑战防护措施所带来的漏洞。本研究评估了大型语言模型在由家长焦虑驱动的对抗压力下，在儿科咨询中的安全性，涉及不同模型和平台。
### Innovation
研究使用了先前评估中的PediatricAnxietyBench，其中包括300个查询（150个真实查询和150个对抗查询），覆盖10个主题。通过API对三种模型进行了评估，分别是Llama-3.3-70B和Llama-3.1-8B（Groq）以及Mistral-7B（HuggingFace），生成了900个响应。通过配对t检验与自助法置信区间分析了安全性评价结果。研究发现了不同模型和平台的安全性表现，并强调了对抗鲁棒性演变及其与规模的关系。
### Conclusion
研究结论指出，安全性取决于对齐和架构而非规模。小型模型的表现优于大型模型。不同版本的演化表明需要有针对性的训练进展以提升鲁棒性。存在漏洞和未识别紧急情况表明，这些模型不适合用于分诊。研究结果有助于选择模型、强调对抗测试，并提供了一个可用于医疗AI安全的开放基准。
## 16. `cs.CL` - ADMEDTAGGER: 用于波兰医学语言知识精炼的注释框架 [PDF](https://arxiv.org/pdf/2601.09722), [HTML](https://arxiv.org/abs/2601.09722)
### Authors
Franciszek Górski,Andrzej Czyżewski
### Background
本文研究背景在于，在大规模医学文本中存在标注资源有限的问题。ADMEDVOICE项目收集了大量代表五个临床类别的医学文本，但标注资源有限使得不能充分标注所有文本。因此，研究利用一个多语言预训练模型（如Llama3.1）进行少量标注资源的高效标注，并利用标注后的数据训练不同类型的基于BERT的分类器。
### Innovation
研究创新点在于提出了一种新的注释框架（ADMEDTAGGER），该框架利用多语言预训练模型对大量波兰医学文本进行标注，并使用这些标注数据训练不同类型的BERT分类器。通过利用少量的标注资源，即使仅验证了一部分标注结果，也能高效地进行模型训练并取得良好的分类性能。
### Conclusion
通过对ADMEDTAGGER所训练的不同类型的BERT分类器进行评估，发现DistilBERT模型取得了最好的效果，各个临床类别F1得分均超过0.80，其中3个类别F1得分超过0.93。这一方法提供了一种替代大型语言模型的可能，具有近500倍的小模型尺寸、300倍更低的GPU VRAM消耗和几倍于传统模型的快速推理速度。
## 17. `cs.CL` - SALP-CG: 标准对齐的大语言模型管道，用于分类和评估大量在线交流医疗数据 [PDF](https://arxiv.org/pdf/2601.09717), [HTML](https://arxiv.org/abs/2601.09717)
### Authors
Yiwei Yan,Hao Li,Hua He,Gong Kai,Zhengyi Yang,Guanfeng Liu
### Background
在线医疗咨询生成了大量的对话型健康数据，这些数据中经常包含了受保护的健康信息。对这类数据实施敏感性分类和风险等级评估，以符合政策和实践要求是必要的。然而，现有的方法缺乏统一的标准和可靠的自动化方法来满足此类对话型健康数据的敏感性分类需求。
### Innovation
该研究提出了一种基于大语言模型的提取流水线—SALP-CG，用于在在线交互式健康数据中分类和评定隐私风险。该流水线采用少量示例指导、JSON模式受限解码及确定性高风险规则相结合的方法，实现了与多样化大语言模型兼容且灵敏度可靠的分类结果。在MedDialog-CN基准测试中，模型表现出了稳健的实体计数、高模式符合度及准确的灵敏度评级，最强模型在最高级别预测中的微调-1值达到了0.900。
### Conclusion
SALP-CG流水线能够可靠地在不同的大语言模型中分类和评估在线交互式健康数据中的类别和灵敏度等级，提供了一种实用的方法来管理医疗健康数据。已将代码开源并发布到指定链接。
## 18. `cs.CL` - Bounded Hyperbolic Tangent: A Stable and Efficient Alternative to Pre-Layer Normalization in Large Language Models [PDF](https://arxiv.org/pdf/2601.09719), [HTML](https://arxiv.org/abs/2601.09719)
### Authors
Hoyoon Byun,Youngjun Choi,Taero Kim,Sungrae Park,Kyungwoo Song
### Background
预层规范化（Pre-LN）是大型语言模型（LLMs）的默认选择，对于稳定的预训练和有效的迁移学习至关重要。然而，Pre-LN由于重复的统计计算而效率低下，并且受到深度诅咒的影响。随着层数的增长，隐藏状态的幅度和方差增加，导致训练不稳。为了提高效率，动态双曲正切（DyT）等去归一化方法虽然加快了速度，但在深度下仍然脆弱。为了解决深度中的稳定性和效率问题，本文提出了有界双曲正切（BHyT），这是一种预层规范化的直接替代方法。
### Innovation
BHyT 将双曲正切非线性与显式的数据驱动输入截断结合，以保持激活值在非饱和范围内。它防止了激活幅度和方差的深度增长，并提供了理论上的稳定性保证。为了提高效率，BHyT 每个块只计算一次精确统计，并用轻量级方差近似替换第二个规范化步骤。实验表明，BHyT 在预训练期间表现出更好的稳定性和效率，平均快15.8%的训练时间，并且生成吞吐量平均提高4.2%，同时在语言理解和推理基准测试中匹配或超越其推理性能和鲁棒性。
### Conclusion
BHyT 在大型语言模型中提供了一种稳定和高效的替代方案，相较于RMSNorm，它在预训练期间提高了稳定性和效率，同时保持或超越了推理性能和鲁棒性。
## 19. `cs.CL` - 不确定性动态知识图谱在可靠问题回答中的应用 [PDF](https://arxiv.org/pdf/2601.09720), [HTML](https://arxiv.org/abs/2601.09720)
### Authors
Yu Takahashi,Shun Takeuchi,Kexuan Xin,Guillaume Pelat,Yoshiaki Ikai,Junya Saito,Jonathan Vitale,Shlomo Berkovsky,Amin Beheshti
### Background
问题回答（QA）系统在各个领域中的应用越来越广泛，但在检索的证据不完整、嘈杂或不确定时，其可靠性会受到影响。现有的基于知识图谱（KG）的QA框架通常将事实表示为静态和确定性的，未能捕捉信息的演变性质和推理中的不确定性。
### Innovation
本文提出了一种不确定性动态KG框架，该框架结合了(i) 动态构建演变中的KG，(ii) 配置置信度评分和不确定性感知的检索，以及(iii) 交互式界面以实现可靠且可解释的QA。系统展示了不确定性建模如何使QA更具鲁棒性和透明度，使用户能够探索动态图谱、检查带有置信度标注的三元组，以及比较基准答案和基于置信度的答案。
### Conclusion
我们在临床数据科学家和临床医生的目标用户中展示了该框架的实际应用，并在医疗保健领域进行了实例，包括从电子健康记录构建个性化KG、跨患者访问诊断不确定性可视化以及评估其对死亡率预测任务的影响。此外，该应用案例展示了不确定性动态KG在高风险应用中增强QA可靠性的广泛前景。
## 20. `cs.CL` - 引入Axlerod：基于LLM的辅助独立保险代理人聊天机器人 [PDF](https://arxiv.org/pdf/2601.09715), [HTML](https://arxiv.org/abs/2601.09715)
### Authors
Adam Bradley,John Hastings,Khandaker Mamun Ahmed
### Background
保险行业正通过采用人工智能（AI）技术经历范式转变，尤其是在智能对话代理领域。聊天机器人已经进化为能够自动化复杂工作流程（包括保险建议和理赔处理）的先进AI系统，同时提供动态且上下文相关的用户交互。
### Innovation
本文设计、实现并实证评估了Axlerod，这是一种基于AI的对话界面，旨在提高独立保险代理的操作效率。Axlerod利用自然语言处理（NLP）、检索增强生成（RAG）和领域特定知识集成，展示了强大的解析用户意图、访问结构化政策数据库以及提供实时、上下文相关响应的能力。
### Conclusion
实验结果验证了Axlerod的有效性，实现了93.18%的理赔检索任务准确率，并将平均搜索时间减少了2.42秒。本研究为保险金融科技中的企业级AI应用领域增添了研究贡献，特别关注于代理辅助而非面向消费者的应用架构。
