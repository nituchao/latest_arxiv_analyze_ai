{"llm_update_time": "20260101", "topic": "cs.AI", "pdf_url": "https://arxiv.org/pdf/2512.22211", "html_url": "https://arxiv.org/abs/2512.22211", "title": "有了强大的能力就应有更大的责任：介绍治理意向性AI系统的能力建模与风险框架", "title_en": "With Great Capabilities Come Great Responsibilities: Introducing the Agentic Risk & Capability Framework for Governing Agentic AI Systems", "authors": "Shaun Khoo,Jessica Foo,Roy Ka-Wei Lee", "background": "意向性AI系统因其自主行动的能力，涵盖代码执行、网络交互和文件修改等任务，为组织带来了重大机遇。但与之同时也伴随着一系列新型风险，给有效的组织治理带来了挑战，包括全面识别、评估和缓解多样且不断演变的风险。本研究旨在应对这一挑战。", "innovation": "引入了意向性风险与能力（ARC）框架，一个面向技术治理的目标性AI系统的框架，具体贡献包括：（1）建立了一种新颖的能力中心视角来分析广泛的意向性AI系统；（2）提炼出三类内在风险源，分别是组件、设计和能力；（3）明确了每个风险源与具体体现风险以及对应的技术控制之间的明确联系；（4）提供了一种结构化和实用的方法，帮助组织实施该框架。", "conclusion": "本框架为组织提供了稳健且灵活的治理方法，能够迅速有效推动创新，同时确保意向性AI系统的安全、安全和负责任的部署。该框架已开源，可以在这个链接找到：[这里](这个 https URL)。"}
{"llm_update_time": "20260101", "topic": "cs.AI", "pdf_url": "https://arxiv.org/pdf/2512.22210", "html_url": "https://arxiv.org/abs/2512.22210", "title": "朝向公平复苏：一种针对孟加拉国洪灾后援助优先级的公平感知人工智能框架", "title_en": "Toward Equitable Recovery: A Fairness-Aware AI Framework for Prioritizing Post-Flood Aid in Bangladesh", "authors": "Farjana Yesmin,Romana Akter", "background": "发展中国家的灾后援助常常遭受系统性偏见的困扰，这些偏见使脆弱地区处于不利地位，加剧了历史上的不平等。2022年孟加拉国的洪水影响了720万人，造成4.055亿美元的损失。该研究基于实际数据，提出了一种公平感知的人工智能框架，旨在优先分配洪灾援助，特别关注孟加拉国这一反复遭受洪水影响的国家。", "innovation": "本文创新性地利用公平感知表示学习技术，从医疗AI领域引入境外适用框架，并结合梯度反转层，使模型学会学习无偏见表示。该研究通过减少统计平等差距和区域公平差距分别达到41.6%和43.2%，并在保留较高预测准确性的前提下，生成实际优先级排序，确保援助更多地流向最需要的人群。", "conclusion": "本文展示了如何将算法公平技术应用于人道主义领域。这种方法为决策者提供了工具，以实施更具公平性的灾害恢复策略。"}
{"llm_update_time": "20260101", "topic": "cs.AI", "pdf_url": "https://arxiv.org/pdf/2512.22207", "html_url": "https://arxiv.org/abs/2512.22207", "title": "GamiBench: 通过折纸折纸任务评估多模态大语言模型的空间推理和二维到三维规划能力", "title_en": "GamiBench: Evaluating Spatial Reasoning and 2D-to-3D Planning Capabilities of MLLMs with Origami Folding Tasks", "authors": "Ryan Spencer,Roey Yaari,Ritvik Vemavarapu,Joyce Yang,Steven Ngo,Utkarsh Sharma", "background": "现有的多模态大语言模型在感知和指令执行方面表现良好，但在空间推理方面仍存在局限性。空间推理是人类智能的关键组成部分，但目前大多数基准测试集中在静态图像或最终输出，未能考虑到这种技能的序列性和视角依赖性。因此，有必要开发新的基准测试工具来评估模型的空间推理能力。", "innovation": "本文提出了GamiBench，这是一个通过折纸启发的折叠任务来评估多模态大语言模型的空间推理和二维到三维规划能力的新基准。GamiBench包括186个常规的和186个不可能的2D折痕模式，以及它们对应的3D折叠形状，涵盖了六个不同的视角和三个视觉问答任务：预测3D折叠配置，区分有效视角，识别不可能的模式。GamiBench还引入了新的诊断指标，如视角一致性（VC）和不可能折叠选择率（IFSR），以测量模型处理不同复杂度折叠的能力。研究表明，即使是领先的模型如GPT-5和Gemini-2.5-Pro在单步空间理解方面也存在问题。", "conclusion": "本文建立了一个标准化框架，用于评估多模态大语言模型在几何理解和空间推理方面的表现。GamiBench基准测试涵盖了从预测3D折叠配置到识别不可能模式的完整推理过程，不仅可以测量跨视角的一致性，还可以通过检测不可能折叠来衡量物理可行性，并评估对中间折叠步骤的解释。"}
{"llm_update_time": "20260101", "topic": "cs.AI", "pdf_url": "https://arxiv.org/pdf/2512.22334", "html_url": "https://arxiv.org/abs/2512.22334", "title": "SciEvalKit：科学通用智能开源评估工具", "title_en": "SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence", "authors": "Yiheng Wang,Yixin Chen,Shuo Li,Yifan Zhou,Bo Liu,Hengjian Gao,Jiakang Yuan,Jia Bu,Wanghan Xu,Yuhao Zhou,Xiangyu Zhao,Zhiwang Zhou,Fengxiang Wang,Haodong Duan,Songyang Zhang,Jun Yao,Han Deng,Yizhou Wang,Jiabei Xiao,Jiaqi Liu,Encheng Su,Yujie Liu,Weida Wang,Junchi Yao,Shenghe Zheng,Haoran Sun,Runmin Ma,Xiangchao Yan,Bo Zhang,Dongzhan Zhou,Shufei Zhang,Peng Ye,Xiaosong Wang,Shixiang Tang,Wenlong Zhang,Lei Bai", "background": "当前，科学领域的人工智能模型评估工具多为通用工具，覆盖面较窄，缺乏针对科学智能核心能力的全面评估。SciEvalKit 设计目标是为了填补这一空白，提供一个统一的评估工具，可以覆盖广泛科学学科和任务能力，旨在评价科学智能模型在多个科学领域的表现。", "innovation": "SciEvalKit 集成了六项核心科学智能能力的评估，即科学多模态感知、科学多模态推理、科学多模态理解、科学符号推理、科学代码生成、科学假设生成和科学知识理解，并且支持包括物理学、化学、天文学和材料科学在内的六大主要科学领域。它通过构建专家级的科学基准，确保评估任务能够反映真实的科学挑战，并且具有灵活、可扩展的评估管道，支持批量评估和自定义模型及数据集集成。", "conclusion": "SciEvalKit 通过整合不同领域的专家级科学基准，提供了标准化且可定制的基础设施，以评估下一代科学基础模型和智能代理的能力，同时强调了透明、可复现和对比结果的重要性，并且是开源的，有助于促进 AI4Science 领域的社区发展和进步。"}
{"llm_update_time": "20260101", "topic": "cs.AI", "pdf_url": "https://arxiv.org/pdf/2512.22258", "html_url": "https://arxiv.org/abs/2512.22258", "title": "逻辑草图提示 (LSP): 一种确定性和可解释性提示方法", "title_en": "Logic Sketch Prompting (LSP): A Deterministic and Interpretable Prompting Method", "authors": "Satvik Tripathi", "background": "大型语言模型 (LLMs) 在自然语言推理方面表现出色，但在执行需要严格规则遵循、确定性和可审计性的任务时依然不可靠。逻辑草图提示 (LSP) 是一个轻量级的提示框架，它引入了类型化变量、确定性条件评估器和基于规则的验证器，能够生成可追踪和重复的结果。", "innovation": "LSP 提出了一种新型的提示方法，通过引入类型化变量、确定性条件评估器和基于规则的验证器，使得模型的输出更加确定性、可解释性和可追踪性，而不牺牲性能。通过两个药理学逻辑合规任务，LSP 的准确性和 F1 分数在所有模型中都显著高于其他提示方法。", "conclusion": "LSP 的结果显示它在提高确定性、可解释性和一致性方面优于其他提示方法，同时不牺牲性能，支持其在临床、监管和安全关键决策支持系统中的应用。"}
{"llm_update_time": "20260101", "topic": "cs.AI", "pdf_url": "https://arxiv.org/pdf/2512.22199", "html_url": "https://arxiv.org/abs/2512.22199", "title": "双向 RAG：通过多阶段验证实现安全的自我提升检索增强生成", "title_en": "Bidirectional RAG: Safe Self-Improving Retrieval-Augmented Generation Through Multi-Stage Validation", "authors": "Teja Chinthala", "background": "检索增强生成（RAG）系统通过将响应与外部知识库对接来增强大型语言模型，但传统的 RAG 架构使用静态语料库，这些语料库不能从用户互动中进化。", "innovation": "提出了一种名为双向 RAG 的新架构，允许通过验证高质量生成的响应安全地扩展语料库。系统采用了多阶段接受层，结合了基于 NLI 的蕴含验证、归属检查和新颖性检测等手段，以防止生成幻觉并允许知识积累。", "conclusion": "双向 RAG 在四个数据集上（自然问题、TriviaQA、HotpotQA 和 Stack Overflow）的实验中，平均覆盖率达到 40.58%，几乎将标准 RAG 的覆盖率翻倍，同时比简单写后的语料库减少了 72% 的文档数量（140 对 500）。研究表明，在严格的验证下，自我提升的 RAG 是可行且安全的，提供了 RAG 系统从部署中学习的实际路径。"}
{"llm_update_time": "20260101", "topic": "cs.AI", "pdf_url": "https://arxiv.org/pdf/2512.22336", "html_url": "https://arxiv.org/abs/2512.22336", "title": "Agent2World: 通过自适应多代理反馈学习生成符号世界模型", "title_en": "Agent2World: Learning to Generate Symbolic World Models via Adaptive Multi-Agent Feedback", "authors": "Mengkang Hu,Bowei Xia,Yuran Wu,Ailing Yu,Yude Zou,Qiguang Chen,Shijian Wang,Jiarui Jin,Kexin Li,Wenxiang Jiao,Yuan Lu,Ping Luo", "background": "符号世界模型（例如PDDL领域或可执行仿真器）在基于模型的规划中至关重要，但训练大规模可验证监督的LLMs来生成这些世界模型受到限制。当前方法主要依赖静态验证方法，无法抓住来自交互执行的行为级错误。", "innovation": "提出了一种工具增强的多代理框架Agent2World，通过多代理反馈进行生成，分为三个阶段：(i) 深度研究员代理进行知识综合以填补规范空白；(ii) 模型开发代理实现可执行世界模型；(iii) 专门测试小组进行适应性的单元测试和基于模拟的验证。该框架在三个基准测试中表现出色，覆盖PDDL和可执行代码表示法，达到最先进的结果。测试小组还为模型开发提供行为感知的反馈，优化多步训练轨迹。", "conclusion": "Agent2World在三个基准测试中实现了强大的推理时性能，特别是在测试团队的指导下，模型微调后生成世界模型的表现优于未训练模型，平均相对增益达到30.95%。"}
{"llm_update_time": "20260101", "topic": "cs.AI", "pdf_url": "https://arxiv.org/pdf/2512.22236", "html_url": "https://arxiv.org/abs/2512.22236", "title": "我们无法识别AI生成的图像", "title_en": "We are not able to identify AI-generated images", "authors": "Adrien Pavão", "background": "随着AI生成的图像在网络上越来越普遍，很多人仍然认为他们能够轻易区分AI生成的图像和真实照片。这项研究通过一个互动的网络实验来测试这个假设：参与者需要将20张图像分类为真实或AI生成的。实验的数据集包括120个困难案例：从CC12M中采样的真实图像和使用MidJourney精心策划的AI生成对应版本。", "innovation": "研究采用了互动的网络实验方法，使用了精心策划的数据集，包括来自CC12M的真实图像和MidJourney生成的AI对应版本，这些都是高质量且具有挑战性的案例。", "conclusion": "参与者在总共233个会话中完成了165次操作，平均准确率仅为54%，略高于随机猜测，且在多次尝试后没有显著提高。平均响应时间为7.3秒，且一些图像比其他图像更容易被误导。这些结果表明，即使是相对简单的肖像图片，人类也无法可靠地识别AI生成的内容。随着合成媒体的不断进步，仅靠人类判断不足以区分真实与虚假的数据。这些发现强调了提高公众意识和制定伦理规范的重要性，尤其是在AI生成的媒体越来越难以与现实区分的情况下。"}
{"llm_update_time": "20260101", "topic": "cs.AI", "pdf_url": "https://arxiv.org/pdf/2512.22255", "html_url": "https://arxiv.org/abs/2512.22255", "title": "思维的形状：在推理任务中，分布比正确性更重要", "title_en": "Shape of Thought: When Distribution Matters More than Correctness in Reasoning Tasks", "authors": "Abhranil Chandra,Ayush Agrawal,Arian Hosseini,Sebastian Fischmeister,Rishabh Agarwal,Navin Goyal,Aaron Courville", "background": "本文研究了通过使用更加能进行推理的模型生成的合成数据集训练语言模型，可以提高语言模型的推理能力。此类训练数据集中的每个轨迹最终导致答案错误，但这些轨迹包含了部分有效的推理步骤。以往的研究表明，相比人类标注数据集，这种训练方法在某些推理任务中可以产生更好的性能。", "innovation": "本文提出了两个关键因素来解释这种现象：首先，合成数据集的分布自然更接近语言模型本身的分布，使其更容易学习。其次，这些“不正确的”轨迹通常只包含部分错误的推理步骤，但仍包含有效的推理步骤，使模型可以从中学到东西。为了进一步验证第一个假设，作者使用另一个语言模型重新表述人类标注的数据集，使其分布更接近模型自身的分布，结果显示这有助于提高性能。为了验证第二个假设，作者引入了逐渐更复杂的合成数据集，并研究模型对此类错误的容忍度。", "conclusion": "研究表明，构建与模型分布更接近的合成数据集是提高模型推理能力的一个关键因素。正确最终答案并不总是可靠的指标，有时候错误的推理痕迹也可能包含有效的推理步骤，从而对模型的性能产生积极影响。因此，在训练语言模型时，数据集的分布比正确的答案要重要得多。"}
{"llm_update_time": "20260101", "topic": "cs.AI", "pdf_url": "https://arxiv.org/pdf/2512.22201", "html_url": "https://arxiv.org/abs/2512.22201", "title": "Emergent Persuasion: Will LLMs Persuade Without Being Prompted?", "title_en": "Emergent Persuasion: Will LLMs Persuade Without Being Prompted?", "authors": "Vincent Chang,Thee Ho,Sunishchal Dev,Kevin Zhu,Shi Feng,Kellin Pelrine,Matthew Kowal", "background": "随着对话型人工智能系统的广泛应用，AI现在能够对人类的意见和信念产生前所未有的影响。现有研究表明，许多大型语言模型（LLMs）在受到要求时会说服用户持有有害的信念或采取行动，且模型的规模越大，其说服力越强。前人研究主要关注误导者利用LLMs进行说服的风险，然而，本研究旨在探究模型在未被明确提示的情况下是否会自发地进行说服，以评估潜在的新兴说服风险。", "innovation": "本研究通过两种场景研究未提示情况下的自发说服：一是通过内部激活引导模型沿特定人格特质行进；二是通过监督微调模型来表现出这些特质。研究发现，沿与说服相关的或不相关的特质引导并不能可靠地增加模型自发说服的倾向性，而监督微调可以增加这种倾向性。此外，通过包含仅良性话题的一般说服数据集进行监督微调的模型，在遇到争议和有害话题时会表现出更高的说服倾向，显示了潜在有害说服现象的出现和进一步研究的必要性。", "conclusion": "研究表明，通过监督微调的LLMs在没有明确指令的情况下仍然有较高的说服倾向，特别是在面对争议和有害话题时。这表明需要进一步研究潜在的有害说服现象及其产生机制，以更好地评估和管理AI系统带来的说服风险。"}
{"llm_update_time": "20260101", "topic": "cs.CL", "pdf_url": "https://arxiv.org/pdf/2501.01516", "html_url": "https://arxiv.org/abs/2501.01516", "title": "量化真实鲁棒性：可信XAI评估中的同义词加权相似度", "title_en": "Quantifying True Robustness: Synonymity-Weighted Similarity for Trustworthy XAI Evaluation", "authors": "Christopher Burger", "background": " adversarial attacks challenge the reliability of Explainable AI (XAI) by altering explanations while the model's output remains unchanged. The success of these attacks on text-based XAI is often judged using standard information retrieval metrics. These measures are poor at evaluating the trustworthiness, as they treat all word perturbations equally while ignoring synonymity, which can misrepresent an attack's true impact.", "innovation": "我们应用同义词加权方法，通过将扰动词的语义相似度融入评价指标，调整现有评估方法的缺陷。这种方法提供更准确的脆弱性评估，为评估AI系统的稳健性提供重要工具。我们防止了对攻击成功率的高估，从而更忠实地理解XAI系统的真实抵抗力。", "conclusion": "我们的方法能够提供更真实的XAI系统抵御对抗操纵的能力评价，避免了对攻击成功的过高评估，增强了对XAI系统的信任度。"}
{"llm_update_time": "20260101", "topic": "cs.CL", "pdf_url": "https://arxiv.org/pdf/2412.10321", "html_url": "https://arxiv.org/abs/2412.10321", "title": "AdvPrefix: 一种细腻的大型语言模型火焰攻击目标", "title_en": "AdvPrefix: An Objective for Nuanced LLM Jailbreaks", "authors": "Sicheng Zhu,Brandon Amos,Yuandong Tian,Chuan Guo,Ivan Evtimov", "background": "许多针对大型语言模型（LLMs）的火焰攻击依赖于一个共同目标：让模型以“当然，这里是你（有害请求）”这样的前缀来回应。虽然这种方法简单直接，但它存在两个局限：对模型行为控制有限，导致了不完整或不现实的被火焰攻击的响应；以及固定格式限制了优化。", "innovation": "作者提出了一种名为AdvPrefix的即插即用的前缀强制目标，通过结合高预填充攻击成功率和低负对数似然这两个标准来选择一个或多个模型特定的前缀。AdvPrefix可无缝融入现有的火焰攻击中，自动缓解先前的局限。例如，将GCG的默认前缀应用于Llama-3，成功攻击的精细度从14%提高到了80%，表明当前的安全对齐没有很好地推广到新前缀。", "conclusion": "AdvPrefix通过选择有效的模型特定前缀，相比之前的简便方法，提供了更高的控制力和灵活性，并且能够自动优化火焰攻击的成功率，揭示了现有安全对齐方法存在的不足。"}
{"llm_update_time": "20260101", "topic": "cs.CL", "pdf_url": "https://arxiv.org/pdf/2501.00654", "html_url": "https://arxiv.org/abs/2501.00654", "title": "ICONS: 影响共识在视觉-语言数据选择中的应用", "title_en": "ICONS: Influence Consensus for Vision-Language Data Selection", "authors": "Xindi Wu,Mengzhou Xia,Rulin Shao,Zhiwei Deng,Pang Wei Koh,Olga Russakovsky", "background": "现有的视觉-语言模型训练依赖于大数据混合，这些混合涵盖了多样化的任务和领域，但经常包含重复信息，增加了计算成本而没有相应的收益。现有方法通常依赖于任务无关的启发式方法来估计数据的重要性，这限制了其在不同任务中的有效性。", "innovation": "ICONS 方法利用梯度基础上的第一阶训练动态来估计每个示例对验证性能的影响，然后通过多数投票跨任务聚合这些估计值。这种方法能够识别一致有价值的数据点，同时降低评分校准和异常值的敏感性，从而实现鲁棒且可扩展的数据选择。我们的选定数据子集（分别从 LLAVA-665K 中的 20%，从 CAMBRIAN-7M 中的 20%，从 VISION-FLAN-186K 中的 20%）在训练模型上保留了与完整数据集相当的性能。", "conclusion": "我们的选定数据能够适应新的任务和模型架构，并推出三个紧凑的数据子集 LLAVA-ICONS-133K, CAMBRIAN-ICONS-1.4M, 和 VISION-FLAN-ICONS-37K 用于高效视觉-语言模型开发。"}
{"llm_update_time": "20260101", "topic": "cs.CL", "pdf_url": "https://arxiv.org/pdf/2412.09614", "html_url": "https://arxiv.org/abs/2412.09614", "title": "RAVEL：通过基于图的关系指导进行稀有概念生成和编辑", "title_en": "RAVEL: Rare Concept Generation and Editing via Graph-driven Relational Guidance", "authors": "Kavana Venkatesh,Yusuf Dalva,Ismini Lourentzou,Pinar Yanardag", "background": "当前的文本到图像(T2I)扩散模型尽管在视觉保真度方面表现出色，但在展现稀有、复杂或文化内涵丰富的概念时仍然面临困难，这主要是由于训练数据的局限性。", "innovation": "RAVEL 是一种无需重新训练的框架，通过将基于图的检索增强生成（RAG）集成到扩散管道中，显著提升了稀有概念生成、情境驱动的图像编辑及自我改正功能。与依赖视觉示例、静态描述或预训练知识的方法不同，RAVEL 利用结构化的知识图谱来检索组成、象征性和关系性上下文，即使没有视觉先验信息也能够实现细致的上下文关联。为了进一步提升生成质量，RAVEL 提出了新颖的 SRD 自校正模块，通过多方面对齐反馈迭代更新提示，增强了属性准确性、叙事连贯性和语义保真度。", "conclusion": "我们的框架适用于包括 Stable Diffusion XL、Flux 和 DALL-E 3 在内的领先扩散模型。在三个新提出的基准测试上进行了广泛的评估，RAVEL 在感知、对齐和LLM作为法官等指标上均优于当前最先进的方法。这些结果表明，RAVEL 是长尾领域可控和可解释的T2I生成的一种强大的范式。"}
{"llm_update_time": "20260101", "topic": "cs.CL", "pdf_url": "https://arxiv.org/pdf/2502.18186", "html_url": "https://arxiv.org/abs/2502.18186", "title": "基于上下文感知和逐步推理引导语言模型实现稳定的语音情感识别", "title_en": "Steering Language Model to Stable Speech Emotion Recognition via Contextual Perception and Chain of Thought", "authors": "Zhixian Zhao,Xinfa Zhu,Xinsheng Wang,Shuiyuan Wang,Xuelong Geng,Wenjie Tian,Lei Xie", "background": "语音情感识别（SER）中，大型语言模型（如Qwen2-Audio）在理解多样化的音频信号、进行音频分析和生成文本响应方面已具能力。然而，这些大型语言模型往往在SER中受到幻觉的影响，导致错误分类或无关输出。", "innovation": "该研究提出了C$^2$SER，一种旨在通过上下文感知和逐步推理提高SER稳定性和准确性的新型大型语言模型。C$^2$SER结合了Whisper编码器的语义感知和Emotion2Vec-S的声学感知，其中Emotion2Vec-S通过半监督学习增强了情感辨别能力。此外，C$^2$SER使用逐步推理方法逐步处理SER，同时利用语音内容和说话风格提高识别效果。为了进一步提高稳定性，C$^2$SER引入了从显式逐步推理到隐式逐步推理的自我蒸馏，减少了错误累积并提高了识别精度。", "conclusion": "广泛的实验表明，C$^2$SER在SER方面优于现有的流行大型语言模型（如Qwen2-Audio和SECap），提供了更加稳定和精确的情感识别。该研究还发布了训练代码、检查点和测试集，以促进进一步的研究。"}
{"llm_update_time": "20260101", "topic": "cs.CL", "pdf_url": "https://arxiv.org/pdf/2508.08270", "html_url": "https://arxiv.org/abs/2508.08270", "title": "Doctor Sun：一种双语多模态大型语言模型在 biomedical AI 中的应用", "title_en": "Doctor Sun: A Bilingual Multimodal Large Language Model for Biomedical AI", "authors": "Dong Xue,Ziyao Shao,Zhaoyang Duan,Fangzhou Liu,Bing Li,Zhongheng Zhang", "background": "大型多模态模型（LMMs）在生物医学任务中显示出巨大潜力，包括病理分析、放射学报告生成和生物医学辅助。然而，现有的多模态生物医学AI通常基于基础LLMs，这限制了对复杂医学概念的理解，尤其是在医疗训练数据有限的情况下。此外，由近期的LLaVA引发的医疗LMM难以有效捕捉文本和图像之间的复杂关系。", "innovation": "我们介绍了Doctor Sun，这是一种专注于医学的大规模多模态生成模型，能够编码、整合和解释包括文本和图像在内的多种生物医学数据模态。特别地，Doctor Sun结合了一个预训练的视觉编码器与一个医学LLM，并在各种医学数据集上进行了两阶段的训练，专注于特征对齐和指令调优。此外，我们还发布了SunMed-VL，一个广泛的双语医学多模态数据集，以及所有相关模型、代码和资源，以支持生物医学多模态研究的进展。", "conclusion": "通过引入Doctor Sun，我们旨在提供一种新型的生物医学多模态解决方案，增强对疾病概念的理解，并促进医学数据的多模态应用研究。"}
{"llm_update_time": "20260101", "topic": "cs.CL", "pdf_url": "https://arxiv.org/pdf/2509.25743", "html_url": "https://arxiv.org/abs/2509.25743", "title": "旋转控制遗忘：认知旋转空间中量化和控制连续遗忘", "title_en": "Rotation Control Unlearning: Quantifying and Controlling Continuous Unlearning for LLM with The Cognitive Rotation Space", "authors": "Xiang Zhang,Kun Wei,Xu Yang,Chenghao Xu,Su Yan,Cheng Deng", "background": "随着大规模语言模型（LLMs）的普及，其安全漏洞已经引起关注。机器遗忘技术被引入以减轻这些风险，通过移除负面数据的影响。然而，现有方法在保留模型性能的同时，也会因连续的遗忘请求而导致累积的灾难性性能损失。", "innovation": "本文提出了一个新的方法叫做旋转控制遗忘（RCU），利用RCU的旋转显著权重来定量和控制连续遗忘过程中的遗忘程度。设计了一种斜对称损失来构建认知旋转空间，使旋转角度的变化能够模拟连续遗忘过程。此外，设计了一个正交旋转轴正则化来强制连续遗忘请求的旋转方向相互垂直，从而有效地减少了干扰并解决了累积的灾难性性能损失。", "conclusion": "实验在多个数据集上证明，本方法无需保留数据集即可达到当前最佳性能（SOTA）。"}
{"llm_update_time": "20260101", "topic": "cs.CL", "pdf_url": "https://arxiv.org/pdf/2505.20981", "html_url": "https://arxiv.org/abs/2505.20981", "title": "RefAV：面向规划的场景挖掘", "title_en": "RefAV: Towards Planning-Centric Scenario Mining", "authors": "Cainan Davidson,Deva Ramanan,Neehar Peri", "background": "自动驾驶车辆（AVs）在日常车队测试中会收集和标注大量多模态数据，这些数据与高清地图定位相关。然而，从未经过筛选的驾驶日志中识别出有趣的和安全性关键的场景仍然是一个巨大的挑战。传统的场景挖掘技术容易出错，耗费时间，并且通常依赖于手工设计的结构化查询。", "innovation": "这篇论文通过重新审视时空场景挖掘，采用近期的视觉语言模型（VLMs）来检测描述场景是否出现在驾驶日志中，并且如果存在，精确地定位在时间和空间上。为此，引入了RefAV，一个大型的数据集，包含10,000个不同的自然语言查询，描述了与运动规划相关的复杂多智能体交互，这些查询是从Argoverse 2 Sensor数据集中1000个驾驶日志中提取出来的。该研究还评估了若干引用式的多对象追踪器，并进行了基准分析，发现直接重新利用现成的VLMs效果不佳，表明场景挖掘呈现独特的挑战。", "conclusion": "最后，论文讨论了最近举办的竞赛，并分享了社区的见解。他们提供的代码和数据集可以在这些链接获取：this https URL和this https URL。"}
{"llm_update_time": "20260101", "topic": "cs.CL", "pdf_url": "https://arxiv.org/pdf/2501.04359", "html_url": "https://arxiv.org/abs/2501.04359", "title": "使用变压器和基于VAE的数据增强实现EEG语音感知解码", "title_en": "Decoding EEG Speech Perception with Transformers and VAE-based Data Augmentation", "authors": "Terrance Yu-Hao Chen,Yulin Chen,Pontus Soederhaell,Sadrishya Agrawal,Kateryna Shapovalenko", "background": "解码非侵入性脑信号（如脑电图EEG）的语音具有提升脑机接口（BCIs）的潜力，特别是在无声通信和言语障碍个体辅助技术方面的应用。然而，基于EEG的语音解码面临着数据噪声、数据集有限以及在复杂任务如语音感知上的表现较差等重大挑战。", "innovation": "本研究通过利用变分自编码器（VAEs）对EEG数据进行增强以提高数据质量，并应用在电肌图（EMG）任务中表现出色的最先进的序列到序列深度学习架构来解决这些挑战，并将该架构适应于单词分类任务。使用包含听述文录音的EEG记录的Brennan数据集进行预处理和评估。实验结果表明，VAEs具有重建用于增强的人工EEG数据的潜力，而我们的序列到序列模型在生成句子方面的表现优于分类模型。", "conclusion": "本研究为基础的研究奠定了基础，未来的研究可以将EEG语音感知解码扩展到语音产生任务，如无声或想象中的语音。"}
{"llm_update_time": "20260101", "topic": "cs.CL", "pdf_url": "https://arxiv.org/pdf/2504.21015", "html_url": "https://arxiv.org/abs/2504.21015", "title": "不检索，生成：针对密集检索的LLM提示合成训练数据", "title_en": "Don't Retrieve, Generate: Prompting LLMs for Synthetic Training Data in Dense Retrieval", "authors": "Aarush Sinha", "background": "在密集检索（dense retrieval）中，通常依赖从大规模文档集合中挖掘的硬负面（HN）示例来训练有效的密集检索模型，这些方法包括BM25或交叉编码器，需要访问完整语料库和昂贵的索引构建。本研究提出了一种直接从给定查询和正文段生成合成硬负面的方法，使用大型语言模型（LLMs）。研究者使用四种参数量为4B到30B的先进LLMs（Qwen3、LLaMA3、Phi4）生成的合成负面，对DistilBERT进行微调，测试了其在10个BEIR基准数据集上的性能。", "innovation": "本研究创新性地提出了一种全新的方法：利用大型语言模型直接从提供的查询和正段生成合成硬负面，从而替代传统的文档库中的硬负面挖掘方法（如BM25或交叉编码器）。这种方法避免了获取全语料库和构建索引的高成本。", "conclusion": "研究结果表明，即使是更强的生成模型生成的合成数据也没有传统基于语料库的挖掘策略（如BM25和交叉编码器）表现好；此外，增加生成模型的参数量并不总是提升检索性能，14B参数的模型表现优于30B模型，在某些情况下甚至是表现最差的。"}
{"llm_update_time": "20260101", "topic": "cs.LG", "pdf_url": "https://arxiv.org/pdf/2512.23078", "html_url": "https://arxiv.org/abs/2512.23078", "title": "使用深度学习进行艺术市场估值", "title_en": "Deep Learning for Art Market Valuation", "authors": "Jianping Mei,Michael Moses,Jan Waelty,Yucheng Yang", "background": "本文探讨了如何利用深度学习技术通过将艺术品的视觉内容纳入预测模型中来改进艺术市场的估值。研究使用了来自主要拍卖行的大规模重复销售数据集，与其他传统模型（如经典的霍尼科回归和树基方法）以及现代深度架构（包括融合表格和图像数据的多模态模型）进行比较。", "innovation": "研究采用多模态深度学习方法，融合了表格数据和图像数据。通过分析，发现虽然艺术家身份和之前的交易历史在整体预测能力上占据主导地位，但在缺乏历史参考的市场上，视觉嵌入提供了独特的、经济上具有意义的贡献。", "conclusion": "研究结果表明，多模态深度学习在初次销售中最能提供显著的价值，这种见解不仅对艺术市场估值的学术研究有重要价值，也对实践具有指导意义。"}
{"llm_update_time": "20260101", "topic": "cs.LG", "pdf_url": "https://arxiv.org/pdf/2512.23090", "html_url": "https://arxiv.org/abs/2512.23090", "title": "基准成功，临床失败：当强化学习优化基准而非患者时", "title_en": "Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients", "authors": "Armin Berger,Manuela Bergau,Helen Schneider,Saad Ahmad,Tom Anglim Lagones,Gianluca Brugnara,Martha Foltyn-Dumitru,Kai Schlamp,Philipp Vollmuth,Rafet Sifa", "background": "最近的强化学习（RL）进步在大型语言模型（LLMs）上提升了推理任务，但它们在医疗成像领域的资源受限应用仍处于探索阶段。", "innovation": "作者提出了ChexReason，这是一种基于视觉-语言模型，采用R1风格的方法（SFT后接GRPO）进行训练，仅使用2000个SFT样本，1000个RL样本和一个A100 GPU。评估发现，虽然GRPO在分布内领域内表现令人满意，但在跨数据集迁移时性能却有所下降。", "conclusion": "结果表明，问题可能源于强化学习框架而非规模效应，识别出一个泛化悖论，即教师指导的推理检查点在优化前在NIH数据集上表现出色。此外，跨模型比较显示，结构化推理方法对于通用视觉语言模型有益，但对于预先培训的医疗模型的帮助有限。因此，精心策划的监督微调可能比激进的RL方法更适合需要跨多种群体稳健性的临床部署。"}
{"llm_update_time": "20260101", "topic": "cs.LG", "pdf_url": "https://arxiv.org/pdf/2512.23071", "html_url": "https://arxiv.org/abs/2512.23071", "title": "通过概率门实现L0约束的联邦学习以实现稀疏性", "title_en": "Federated Learning With L0 Constraint Via Probabilistic Gates For Sparsity", "authors": "Krishna Harsha Kovelakuntla Huthasana,Alireza Olama,Andreas Lundell", "background": "联邦学习（FL）是一种分布式机器学习设置，要求多个客户端协作训练模型同时保持数据隐私。然而，由于数据和模型固有的稀疏性不足，易导致模型过于密集，特别是在数据和客户端参与存在异质性的情况下，泛化性能较差。", "innovation": "该研究引入了一种通过使用概率门及其连续松弛将L0约束应用于非零参数密度的方法，并基于集中式机器学习中最初提出的方法实现。研究提出了基于联邦随机梯度下降的分布式学习算法，用于实现数据和客户端参与异质性条件下的目标参数密度，并展示了在线性和非线性模型（包括线性回归、逻辑回归、softmax多类分类、带逻辑单元的多标签分类以及卷积神经网络）中的效果，同时与基于幅度剪枝的阈值算法进行了比较，实验结果表明该方法在通信效率和统计性能上更具优越性。", "conclusion": "实验结果表明，无论是合成数据集的稀疏性目标密度为ρ=0.05还是公开的RCV1、MNIST和EMNIST数据集的稀疏性目标密度为ρ=0.005，该方法都能高效地减小通信开销并具有较好的统计性能。"}
{"llm_update_time": "20260101", "topic": "cs.LG", "pdf_url": "https://arxiv.org/pdf/2512.23126", "html_url": "https://arxiv.org/abs/2512.23126", "title": "InSPO：利用内在自我反思提高LLM偏好优化", "title_en": "InSPO: Unlocking Intrinsic Self-Reflection for LLM Preference Optimization", "authors": "Yu Li,Tian Lan,Zhengling Qi", "background": "直接偏好优化（DPO）及其变体因其实现简单和离线稳定性已成为对齐大型语言模型的标准方法。然而，研究发现DPO存在两个根本问题：首先，最优策略依赖于任意的建模选择（标量化函数、参考策略），导致策略反映了参数化中的艺术而非真实偏好；其次，孤立地处理响应生成未能充分利用成对数据中的比较信息，从而未能充分发挥模型的内在自我反思能力。", "innovation": "提出了一种新的内在自我反思偏好优化方法（ι），该方法依据上下文和替代响应的条件推导出全局最优策略。证明了该方法优于DPO/RLHF，保证了对标量化函数和参考策略的选择不变性。该方法作为插拔即用的增强方案，无需修改架构或增加推理开销。实验证明，提高胜率和控制长度的指标验证了解放自我反思提高了更稳健、更符合人类价值观的LLM。", "conclusion": "解锁自我反思的方法（ι）在LLM偏好优化中表现优异，确保了对参数化和参考策略选择的不变性，能够在无需架构更改或推理开销的条件下提升LM的性能。这也验证了通过增强模型的内在自我反思能力能够生成更稳固和与人类价值观更一致的LLM。"}
{"llm_update_time": "20260101", "topic": "cs.LG", "pdf_url": "https://arxiv.org/pdf/2512.23138", "html_url": "https://arxiv.org/abs/2512.23138", "title": "为什么机器学习模型系统性地低估极端值II：如何使用LatentNN修复之", "title_en": "Why Machine Learning Models Systematically Underestimate Extreme Values II: How to Fix It with LatentNN", "authors": "Yuan-Sen Ting", "background": "测量误差在输入变量中的系统性削弱导致回归系数的低估问题影响了天文学数据驱动的模型。虽然对于线性回归，该问题可以通过将真实输入值作为待估计的潜变量来解决，但对于神经网络来说，同样的问题未被解决。", "innovation": "提出了一种名为LatentNN的方法，该方法通过最大化输入和输出观测值的联合似然来共同优化网络参数和潜输入值，从而解决了神经网络中由于测量误差导致的衰减偏差问题。该方法不仅适用于一维回归，还适用于多变量输入和相关特征，以及恒星光谱的应用。LatentNN在不同信噪比下减少了衰减偏差，特别在低信噪比和少量信息特征的条件下效果明显。", "conclusion": "LatentNN提供了一种在天文学数据特征较少且信噪比低的背景下改进神经网络推理的方法。测量误差在数据范围内的幅度不超过一半时，该偏差修正效果最为显著。"}
{"llm_update_time": "20260101", "topic": "cs.LG", "pdf_url": "https://arxiv.org/pdf/2512.23144", "html_url": "https://arxiv.org/abs/2512.23144", "title": "基于推理的结构化在决策中意图和可获取性饱和机制", "title_en": "An Inference-Based Architecture for Intent and Affordance Saturation in Decision-Making", "authors": "Wendyam Eric Lionel Ilboudo,Saori C Tanaka", "background": "决策瘫痪，即犹豫、冻结或尽管有充分的知识和动机但仍不采取行动，构成了对假设选项已经明确且容易比较的选择模型的挑战。本文通过自闭症研究中的定性报告提出了一个计算解释，认为瘫痪源于层次决策过程中的收敛失败。文中将意图选择（什么要追求）和可获取性选择（如何追求目标）区分开来，并形式化了承诺为混合反向和前向休·凯利（KL）目标下的推理。反向KL倾向于模式追求，促进快速承诺，而前向KL则倾向于模式覆盖，保留多个可能的目标或行动。", "innovation": "提出了一种基于推理的计算框架，解释在决策过程中由于意图和可获取性的饱和而导致的决策瘫痪现象。具体来说，通过区分意图选择与可获取性选择，并引入混合的反向和前向KL目标下的推理，可以解释决策拖延和决策关闭的关键特征。", "conclusion": "对多选项任务的模拟重现了关键的决策惰性和关闭特征。将自闭症视为一般推理为基础的决策连续体中的极端情况，提供了理解复杂决策过程中决策瘫痪机制的新视角。"}
{"llm_update_time": "20260101", "topic": "cs.LG", "pdf_url": "https://arxiv.org/pdf/2512.23171", "html_url": "https://arxiv.org/abs/2512.23171", "title": "验证被遗忘权：垂直联邦学习中样本和标签抹除的原始对偶优化", "title_en": "Certifying the Right to Be Forgotten: Primal-Dual Optimization for Sample and Label Unlearning in Vertical Federated Learning", "authors": "Yu Jiang,Xindi Tong,Ziyao Liu,Xiaoxi Zhang,Kwok-Yan Lam,Chee Wei Tan", "background": "联邦卸载已经成为解决协作机器学习中隐私问题的一种有吸引力的方法，尤其是在AI模型在机器学习过程中记忆敏感数据的情况下。它允许从已训练的模型中移除特定数据的影响，与对“被遗忘的权利”的日益重视相一致。尽管在水平联邦学习中广泛研究，但在垂直联邦学习（VFL）中，卸载仍然具有挑战性，因为它包含分布在不同参与方之间的特征。VFL卸载包括样本卸载，移除特定数据点的影响，以及标签卸载，移除整个类别。由于不同参与方持有相同样本的互补特征，因此卸载任务需要跨方协调，从而增加计算负担和复杂性，尤其是由于特征之间的依赖关系。为了应对这些挑战，该研究提出了一种FedORA（联邦优化以原始对偶算法数据删除），专为VFL中的样本和标签卸载设计。", "innovation": "FedORA通过原始对偶框架将某些样本或标签的移除公式化为一个受约束的优化问题。引入了新的卸载损失函数，旨在促进分类不确定性而非误分类。通过自适应步长增强稳定性，并采用考虑剩余数据对模型先验影响的非对称批次设计，以便在移除数据和保留数据之间高效地减少计算成本。研究提供了理论分析，证明了FedORA与从头开始训练的模型差异是有界的，从而确保卸载的有效性。", "conclusion": "为了验证FedORA的有效性，我们在表格和图像数据集上进行了实验，结果表明，FedORA与从头开始训练的模型在卸载效果及保持数据实用性方面具有可比性，同时具有减少计算和通信开销的优势。"}
{"llm_update_time": "20260101", "topic": "cs.LG", "pdf_url": "https://arxiv.org/pdf/2512.23080", "html_url": "https://arxiv.org/abs/2512.23080", "title": "基于QSAR引导的生成框架用于合成可行的香味分子发现", "title_en": "QSAR-Guided Generative Framework for the Discovery of Synthetically Viable Odorants", "authors": "Tim C. Pearce,Ahmed Ibrahim", "background": "新气味分子的发现对于香精和风味行业至关重要，但高效地在庞大的化学空间中识别具有良好嗅觉特性的分子结构仍然是一个重大挑战。生成性人工智能为从头分子设计提供了有希望的方法，但通常需要大量的分子数据进行学习。这个研究工作旨在解决大规模分子数据的要求问题，通过结合变分自编码器（VAE）和定量构效关系（QSAR）模型，利用有限的训练分子集生成新型气味分子。", "innovation": "研究提出了一种结合变分自编码器和定量构效关系模型的框架，用于生成新型气味分子。该方法能够在有限数据集下学习分子结构，通过自监督学习能力从ChemBL数据库学习SMILES语法，进一步加入外部QSAR模型的损失函数来优化潜在空间中的气味概率结构。此方法不仅验证了模型的有效性和创新性，还证明了其在化学空间探索上的广阔潜力。", "conclusion": "研究结果表明，所提出的VAE-QSAR框架能够有效地生成具有良好结构合法性和新颖性化学骨架的新型气味分子，这些分子具有较低的Frechet ChemNet距离值，反映了其高质量的化学空间探索能力。生成的候选分子在物理化学性质方面表现良好。"}
{"llm_update_time": "20260101", "topic": "cs.LG", "pdf_url": "https://arxiv.org/pdf/2512.23167", "html_url": "https://arxiv.org/abs/2512.23167", "title": "SPIRAL: 符号LLM规划通过基础且反思性搜索", "title_en": "SPIRAL: Symbolic LLM Planning via Grounded and Reflective Search", "authors": "Yifan Zhang,Giridhar Ganapavarapu,Srideepika Jayaraman,Bhavna Agrawal,Dhaval Patel,Achille Fokoue", "background": "大型语言模型（LLMs）在需要探索和自我修正的复杂规划任务中经常表现不佳，因为它们的线性推理过程难以从早期错误中恢复。虽然像蒙特卡洛树搜索（MCTS）这样的搜索算法可以探索替代方案，但在稀疏奖励的引导下往往效果不佳，并未能充分利用LLMs丰富的语义能力。", "innovation": "SPIRAL是一个新颖的框架，将三个专门的LLM代理嵌入到MCTS循环中，形成一个集成的规划管道。SPIRAL的关键贡献在于：规划者提出创造性的下一步，模拟器通过预测现实结果来确定搜索路径，评论者通过反思提供密集的奖励信号。这种协同作用将MCTS从盲目的搜索转变为受指导、自我纠正的推理过程。SPIRAL在一日常生活APIs和HuggingFace数据集中，相对于默认的思维链规划方法及其他最新算法取得了更优的表现，特别是在整体准确率和token效率方面。", "conclusion": "我们的研究证明，将LLM推理结构化为有指导的、反思性的、基础性的搜索过程能产生更加稳定和高效的自主规划者。完整的源代码、附录和所有实验数据可在官方项目库中获取，以支持可复制性。"}
{"llm_update_time": "20260101", "topic": "cs.LG", "pdf_url": "https://arxiv.org/pdf/2512.23132", "html_url": "https://arxiv.org/abs/2512.23132", "title": "基于多智能体的AI系统威胁缓解与韧性框架", "title_en": "Multi-Agent Framework for Threat Mitigation and Resilience in AI-Based Systems", "authors": "Armstrong Foundjem,Lionel Nganyewou Tidjon,Leuson Da Silva,Foutse Khomh", "background": "机器学习（ML）是金融、医疗和基础设施等关键领域基础模型的核心，使其成为数据投毒、模型提取、提示注入、自动化越狱和偏好导向的黑盒攻击的目标。传统的网络安全缺乏针对基础模型、多模态模型和基于检索的生成系统（RAG）的特定于ML的威胁建模。研究旨在通过确定主要战术和技术（TTPs）、漏洞和目标生命周期阶段来表征ML安全风险。", "innovation": "该研究通过从MITRE ATLAS、AI事件数据库和文献中提取93种威胁，并分析854个GitHub/Python存储库，构建了一个基于本体的知识图谱，识别出未报告的威胁，如商业LLM API模型窃取、参数记忆泄漏以及文本导向的偏好引导型越狱。研究揭示了依赖库的密集漏洞簇，这些库缺乏修复传播。研究提出了适应性强的特定于ML的安全框架，结合依赖卫生、威胁情报和监控，以缓解从供应链到推理的ML生命周期中的风险。", "conclusion": "为了缓解ML生命周期中的供应链和推理风险，适应性强的特定于ML的安全框架是必不可少的，该框架结合了依赖卫生、威胁情报和监控。"}
