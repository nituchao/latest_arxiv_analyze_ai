# 20260123
[![Subscribe_Visitors](https://visitor-badge.laobi.icu/badge?page_id=nituchao.latest_arxiv_analyze_ai_rss)](https://github.com/nituchao/latest_arxiv_analyze_ai)

## 1. `cs.AI` - 在多树遗传编程中具有膝盖点引导的可扩展的活动组选择策略以用于动态多模式项目调度 [PDF](https://arxiv.org/pdf/2601.14485), [HTML](https://arxiv.org/abs/2601.14485)
### Authors
Yuan Tian,Yi Mei,Mengjie Zhang
### Background
动态多模式资源约束项目调度问题要求在活动执行顺序及其对应的执行模式之间做出决策。遗传编程已广泛应用于生成优先级规则，这些规则指导选择当前可执行活动的子集。最近提出了一种活动组选择策略，该策略在每次决策时不单单选择一个活动，而是选择一个活动组，从而通过考虑活动之间的相互依赖性来更有效地进行调度。然而，该方法在处理大规模问题时存在可扩展性不足的问题。
### Innovation
本文通过引入基于膝盖点的选择机制提升了活动组选择策略的可扩展性。首先使用活动排序规则对所有可活动-模式组合进行排序，然后利用膝盖点选取方法找到一组有希望的组合，最后制定活动组合规则选择最佳活动组合。同时采用了多树遗传编程框架来同时进化两种规则。
### Conclusion
实验结果表明，本文方法可很好地应用于大规模实例，并在大多数情况下优于顺序决策的遗传编程方法。
## 2. `cs.AI` - 基于 phylogenetic 树的大型语言模型驱动的演化代码优化 [PDF](https://arxiv.org/pdf/2601.14523), [HTML](https://arxiv.org/abs/2601.14523)
### Authors
Leyi Zhao,Weijie Huang,Yitong Guo,Jiang Bian,Chenghong Wang,Xuhong Zhang
### Background
现代 GPU 上的科学计算算法优化是一个劳动密集型且迭代的过程，涉及反复的代码修改、基准测试和调优，且需要跨复杂硬件和软件栈进行操作。近期研究表明，使用大型语言模型（LLM）辅助的演化方法可以实现自动化的代码优化，但这些方法主要依赖于结果选择和随机突变，未能充分利用迭代优化过程中生成的丰富轨迹信息。
### Innovation
该论文提出了一种名为 PhyloEvolve 的 LLM-代理系统，将 GPU 指向的算法优化问题重新定义为基于上下文的强化学习（ICRL）问题。PhyloEvolve 整合了算法蒸馏和基于提示的决策变换器，将一系列算法修改和性能反馈作为学习信号的第一类。它引入了一种谱系树表示法，用于捕获算法变体之间的继承、分异和重组，从而实现回溯、谱系间转移和可重复性。系统结合了精英轨迹池、多岛并行探索和容器化执行，跨异构硬件实现探索与利用的权衡。
### Conclusion
在偏微分方程求解器、流形学习和谱图算法等科学计算负载上的评估显示，PhyloEvolve 在运行时、内存效率和正确性方面持续优于基线和演化方法。相关代码已发布。
## 3. `cs.AI` - 即时构建世界模型支持人类规划和推理 [PDF](https://arxiv.org/pdf/2601.14514), [HTML](https://arxiv.org/abs/2601.14514)
### Authors
Tony Chen,Sam Cheyette,Kelsey Allen,Joshua Tenenbaum,Kevin Smith
### Background
心理模拟被认为在人类推理、计划和预测中起着关键作用，但在复杂环境中模拟对人类的能力极限提出了挑战。有一种理论认为，人们使用简化的环境表示来避免不必要的细节，但目前尚不清楚人们是如何高效地确定这些简化的过程。
### Innovation
该论文提出了一个“即时构建”框架，以展示如何在线构建简化的表示，仅需少量增加计算即可。模型通过模拟、视觉搜索和表示修改的紧密交织进行工作，当前的模拟指导视觉搜索，而视觉搜索标记需要编码的物体供后续模拟使用，尽管只会对少数物体进行编码，该模型仍能做出高价值预测。
### Conclusion
在网格世界规划任务和物理推理任务中，该研究结果在多种行为措施上为该账户提供了有力的实证支持，并为人们如何构建简化表示以支持高效心理模拟提供了一个具体的算法解释。
## 4. `cs.AI` - MAS-Orchestra：通过整体交响化和受控基准了解和提高多智能体推理 [PDF](https://arxiv.org/pdf/2601.14652), [HTML](https://arxiv.org/abs/2601.14652)
### Authors
Zixuan Ke,Yifei Ming,Austin Xu,Ryan Chin,Xuan-Phi Nguyen,Prathyusha Jwalapuram,Semih Yavuz,Caiming Xiong,Shafiq Joty
### Background
尽管多智能体系统（MAS）通过代理之间的协同工作提供了提高智能的潜力，但当前的自动MAS设计方法尚未充分利用这一潜力。这类系统的核心问题在于方法复杂性和效果不确定性。方法复杂性表现在代理协调是通过顺序、代码级执行完成的，这限制了全局系统级的整体推理能力，并且伴随代理复杂度升高，扩展能力迅速衰减。效果不确定性体现在MAS往往是直接部署，而缺乏对其相比单智能体系统（SAS）是否有实质性的效果提升的明确理解。
### Innovation
本文提出的MAS-Orchestra是一种在训练时间框架，将MAS协调形式化为一种涵盖整体协调功能调用强化学习问题的方法，可以一次性生成整个MAS。在MAS-Orchestra中，复杂的、具有目标导向的子代理被视为可调用的函数，这使得系统结构的整体推理成为可能，同时隐藏了内部执行细节。此外，为了系统地研究何时及为何MAS有益，作者引入了MASBENCH，这是一个综合基准，从深度、视距、广度、并行性和稳健性五个维度来表征任务。文章分析表明，MAS的效益取决于任务结构、验证协议以及交响者和子代理的能力。
### Conclusion
受到这些见解的启示，MAS-Orchestra在公开基准测试中（如数学推理、多跳QA和基于搜索的QA）实现了持续的改进。MAS-Orchestra和MASBENCH共同有助于在追求多智能体智能的过程中更好地培训和理解多智能体系统。
## 5. `cs.AI` - Query-Efficient Agentic Graph Extraction Attacks on GraphRAG Systems [PDF](https://arxiv.org/pdf/2601.14662), [HTML](https://arxiv.org/abs/2601.14662)
### Authors
Shuhua Yang,Jiahao Zhang,Yilong Wang,Dongwon Lee,Suhang Wang
### Background
GraphRAG系统利用文档集合构建知识图谱以支持多跳推理。尽管早期工作表明GraphRAG的响应可能泄露检索的子图，但在真实查询预算下，隐藏的图结构能否高效重建尚未被研究。本文探讨了一个查询预算约束下的黑盒环境，其中攻击者适应性地查询系统以窃取其潜在的实体关系图。
### Innovation
本文提出了一种名为AGEA（Agentic Graph Extraction Attack）的框架，它利用新颖导向的探索利用策略、外部图记忆模块以及结合轻量级发现与基于LLM的过滤的两阶段图提取流水线。通过在医疗、农业和文学数据集上使用Microsoft-GraphRAG和LightRAG系统进行评估，AGEA在相同查询预算下显著优于现有基准攻击，能够复现高达90%的实体和关系同时保持高精度。
### Conclusion
本文结果表明，现代GraphRAG系统在严格查询限制下对结构化的、有目的性的提取攻击极为脆弱。
## 6. `cs.AI` - 使用本地语言模型实现敏感文本的上下文感知自适应匿名化 [PDF](https://arxiv.org/pdf/2601.14683), [HTML](https://arxiv.org/abs/2601.14683)
### Authors
Aisvarya Adeseye,Jouni Isoaho,Seppo Virtanen,Mohammad Tahir
### Background
定性研究常包含个人、情境和组织的细节，容易泄露隐私。手动匿名化耗时且不一致，而现有自动化工具依赖于模式匹配或固定规则，无法捕捉上下文，可能导致数据意义改变。本研究使用本地语言模型构建了一个可靠的、可重复的、上下文感知的匿名化过程，以检测和匿名化质性转录中的敏感数据。
### Innovation
引入了一个基于策略的框架（SFAA），包括检测、分类和自适应匿名化三个步骤。SFAA采用了四种策略：基于规则的替代、上下文感知重写、泛化和删除。这些策略根据标识符类型和风险级别应用，所有标识符处理遵循GDPR、HIPAA和OECD的国际隐私和研究伦理标准。通过结合手动和LLM辅助处理的双重方法进行评估。
### Conclusion
研究的LLMs比人工审查员发现了更多的敏感数据，Phi模型在发现敏感数据方面优于LLaMA，但在错误方面略高，但Phi能够发现超过91%的敏感数据，并保持94.8%的原始情感赞赏度，因此对定性数据的分析影响极小。
## 7. `cs.AI` - VisTIRA：通过结构化的工具整合来消除视觉数学推理中的图像-文本模态鸿沟 [PDF](https://arxiv.org/pdf/2601.14440), [HTML](https://arxiv.org/abs/2601.14440)
### Authors
Saeed Khaki,Ashudeep Singh,Nima Safaei,Kamal Ginotra
### Background
视觉语言模型（VLMs）在以图像形式呈现的问题上，在数学推理方面落后于仅文本的语言模型。研究发现，同一问题以文本形式呈现时的准确性远高于其以文字排版的视觉版本，这归因于阅读密集公式、排版和混合符号-图表上下文时的一系列失败。
### Innovation
1. VisTIRA（视觉和工具集成推理代理）：一种工具集成推理框架，通过逐步将给定的数学问题（作为图像）分解成自然语言推理和可执行的Python步骤，来确定最终答案，从而实现结构化问题解决。2. 构建了一个框架来测量和改进视觉数学推理：基于LaTeX的管道将链式思维的数学语料库（例如，NuminaMath）转换为具有挑战性的图像版本，并从一个现实世界的家庭作业风格的图像数据集（称为SnapAsk）中衍生出大量合成的工具使用轨迹，以细化VLMs。3. 通过工具整合监督可以提高图像推理，而OCR定位可以进一步缩小较小模型的差距，但其益处随着模型规模的增大而减小。这些发现表明，模态鸿沟的严重程度与模型规模呈反相关，并且结构化推理和基于OCR的定位是推进视觉数学推理的互补策略。
### Conclusion
这些研究结果强调，模态鸿沟的严重程度与模型规模呈反比关系，结构化推理和基于OCR的定位是推进视觉数学推理的互补策略。
## 8. `cs.AI` - 《本体中立性定理：为什么中立的本体基础必须是超因果性和超规范性》 [PDF](https://arxiv.org/pdf/2601.14271), [HTML](https://arxiv.org/abs/2601.14271)
### Authors
Denise M. Case
### Background
现代数据分析系统必须支持在持续的法律、政治和分析分歧中的问责制。这种需求对旨在作为共享底层结构运行的本体设计提出了严格的约束。本文探讨了本体中立性的限制，即在底层引入因果性和规范性承诺与本体中立性的矛盾。
### Innovation
本文确立了一个中立本体中立性定理，指出任何声称因果或规范结论作为本体事实的本体，在不同框架中都不能保持中立，除非进行修订或产生矛盾。因此，中立的本体基础必须是超因果性和超规范性的，仅描述实体及其身份和持久性条件，而将解释、评估和解释外部化。
### Conclusion
此论文并没有提出特定的本体或协议，而是为任何旨在维护不同解释框架下共享、稳定现实表示的系统建立了必要的设计约束。
## 9. `cs.AI` - LLM规划中的泛化缺口：测试与验证奖励RL [PDF](https://arxiv.org/pdf/2601.14456), [HTML](https://arxiv.org/abs/2601.14456)
### Authors
Valerio Belcamino,Nicholas Attolino,Alessio Capitanelli,Fulvio Mastrogiovanni
### Background
近期研究表明，微调的大型语言模型（LLMs）可以在PDDL规划任务中取得较高有效的计划成功率。然而，这一成功究竟是反映了一般的规划能力还是特定领域的记忆尚不清晰。本研究旨在探讨这一点。
### Innovation
作者细调了一个包含1.7B参数的LLM在10个IPC 2023领域中的40,000个领域-问题-计划元组上，并评估其在领域内和跨领域的泛化能力。还引入了三种诊断干预措施：实例级别的符号匿名化、紧凑型计划序列化以及使用VAL验证器作为以成功为导向的强化信号进行验证奖励微调。
### Conclusion
尽管模型在领域内达到了82.9%的有效计划成功率，但在两个未知领域中却完全失败。符号匿名化和紧凑序列化尽管能保留计划语义，却导致显著性能下降，表明对表面表示的高度敏感性。验证奖励微调在短短一半的监督训练周期内达到性能极限，但未改善跨领域的泛化能力。无论配置如何，在领域的泛化性能约在80%周围达到顶峰，而跨领域性能则崩溃，这暗示本研究中的微调模型主要依赖于特定领域的模式而不是转移的规划能力。本研究的结果强调了LLM在基于规划中的持续泛化缺口，并提供了解析其原因的诊断工具。
## 10. `cs.AI` - 知识宪法或：如何避免一致性偏差 [PDF](https://arxiv.org/pdf/2601.14295), [HTML](https://arxiv.org/abs/2601.14295)
### Authors
Michele Loi
### Background
大型语言模型越来越充当人工推理者，它们评估论点，赋予可信度，并表达信心。然而，这些系统形成和表达信念的行为是由未检验的、隐含的客观性政策所引导的。文章提出了为AI构建客观性宪法的想法，即明确的、可争议的元规范，规范系统形成和表达信念的方式。文章以出处归因偏差作为案例，指出前沿模型通过确保身份立场的一致性来惩罚与论点内容冲突的预期意识形态立场的来源。在检测系统测试的情况下，这种影响会消失，揭示了系统将出处敏感性视为一种需要抑制的偏见，而不是执行有效推理的能力。文章区分了两种宪法方法：柏拉图主义方法，它要求从特权立场出发的正式正确性和默认的出处独立性；自由主义方法，它拒绝这种特权，而是在保护集体探究条件的同时，允许基于知识警觉的有原则的出处关注。
### Innovation
文章强调了构建AI客观性宪法的重要性，提出了两种宪法方法——柏拉图主义和自由主义，并选择了自由主义方法。文章还提出了一套八项原则和四项方向的核心宪法结构，并建议人工智能的客观性治理需要同样的明确和可争议的结构，这是目前对于AI伦理的期望。这为如何避免一致性偏见提供了一种新的视角和方法。
### Conclusion
文章得出结论支持自由主义方法，提出了一项核心宪法结构中的八项原则和四项方向，并强调这种结构对于AI客观性治理的重要性。
## 11. `cs.CL` - 自盲和反事实自我模拟减轻大型语言模型中的偏见与逢迎 [PDF](https://arxiv.org/pdf/2601.14553), [HTML](https://arxiv.org/abs/2601.14553)
### Authors
Brian Christian,Matan Mazor
### Background
公平决策需要忽略无关或可能产生偏见的信息。这需要决策者在不知道某些事实，如申请人的性别或种族的情况下，来模拟他们将如何做出决策。然而，这种反事实的自我模拟对于人类来说极具挑战性，即使是有善意的决策者也可能产生偏见。该研究发现，大型语言模型（LLMs）在反事实的自我模拟中也存在着类似的限制，这使得它们在减少性别、种族偏见和避免逢迎方面表现出与人类相似的局限性。
### Innovation
研究发现，提示模型忽略或假装不知道有关偏见的信息并不能有效减轻偏见，有时甚至会导致负面效果。然而，与人类不同，大型语言模型可以访问它们自己的反事实认知的真实模型——它们自己的API。研究证明，通过访问盲复制体的响应，可以做出更加公平的决策，并且能够区分隐性的和故意的偏见行为。
### Conclusion
通过使大型语言模型能够获取到它们自己的反事实认知的真实模型，能够减轻偏见并提升透明度，从而区分隐性的和故意的偏见行为。
## 12. `cs.CL` - 奖励模型的教育性思考：教育LLM中结合教育性推理和思考奖励 [PDF](https://arxiv.org/pdf/2601.14560), [HTML](https://arxiv.org/abs/2601.14560)
### Authors
Unggi Lee,Jiyeong Bae,Jaehyeon Park,Haeun Park,Taejun Park,Younghoon Jeon,Sungmin Cho,Junbo Koh,Yeil Jeong,Gyeonggeon Lee
### Background
大型语言模型（LLMs）正在被越来越多地用于智能辅导系统，但在针对教育环境优化LLMs方面的研究仍然有限。尽管最近的研究提出了使用强化学习方法训练LLM辅导系统，但这些方法主要关注优化可见的响应，而忽视了模型内部的思考过程。
### Innovation
引入了PedagogicalRL-Thinking框架，该框架通过两种创新方法扩展了面向教育环境中的推理LLM（1）基于特定学科的教育理论的教育性推理提示，而不是使用通用指令；（2）思考奖励，它明确评价并强化模型推理痕迹的教育质量。
### Conclusion
研究表明，特定领域的理论支持提示优于通用提示，而思考奖励与教育提示结合使用效果最佳。仅在数学辅导对话上进行训练的模型，在教育基准测试中表现出改进的表现，同时保留了基础模型的事实知识。定量和定性的分析表明，教育性思考奖励产生了一系列系统性的推理痕迹变化，增强了教育性推理，并使辅导者的思考过程更为结构化和系统化。
## 13. `cs.CL` - 朝向基于执行的自动化AI研究 [PDF](https://arxiv.org/pdf/2601.14525), [HTML](https://arxiv.org/abs/2601.14525)
### Authors
Chenglei Si,Zitong Yang,Yejin Choi,Emmanuel Candès,Diyi Yang,Tatsunori Hashimoto
### Background
自动化AI研究具有加速科学发现的巨大潜力。然而，当前的语言模型（LLMs）常常产生看似合理的但效果不佳的想法。执行接地可能有所帮助，但不清楚自动化执行是否可行，以及LLMs是否可以从执行反馈中学习。
### Innovation
本研究构建了一个自动化执行器来实现想法，并使用大规模并行GPU实验验证其有效性。研究将两个实际研究问题——LLM预训练和后训练——转化为执行环境，并表明自动化执行器可以实现大量源自前沿LLM的创意。研究还分析了从执行反馈中学习的两种方法：进化搜索和强化学习。进化导向的搜索效率较高，在后训练中找到了显著优于GRPO基准的方法（69.4% vs 48.0%），并在预训练中找到了优于nanoGPT基准的食谱（19.7分钟 vs 35.9分钟）。然而，强化学习从执行奖励中则遇到了模式崩溃的问题，模型最初集中在简单想法上，从而未能提高上界。研究还详细分析了执行想法和训练动态，为未来的基于执行的自动化AI研究提供了助力。
### Conclusion
前沿LLMs在搜寻过程中经常产生有意义的算法性想法，但往往过早饱和，仅偶尔表现出扩展趋势。相比之下，强化学习从执行奖励中则会陷入模式崩溃。这些发现有助于未来对基于执行的自动化AI研究的进一步努力。
## 14. `cs.CL` - 大语言模型的推理能力是否可靠？——基于统计任务的人类基准测试比较研究 [PDF](https://arxiv.org/pdf/2601.14479), [HTML](https://arxiv.org/abs/2601.14479)
### Authors
Crish Nagarkar,Leonid Bogachev,Serge Sharoff
### Background
最新的大语言模型（LLMs）在自然语言处理（NLP）任务中表现出色，但它们在处理中等复杂度的统计挑战方面的能力尚未得到充分理解。本研究探讨了LLMs解决统计任务的能力，以及它们评估推理质量的能力。
### Innovation
研究对开源的选定LLMs进行了微调，并使用专用数据集提高其统计推理能力，将其性能与作为基准的人类评分进行比较。结果显示，微调后的模型在复杂统计任务上的表现接近统计专业学生水平，且表现出架构相关的改进，一些模型显示出显著的性能提升。此外，研究还发现，LLMs可以比传统的BLEU或BertScore等指标更准确地评估答案的质量，包括解释和推理的评估，这使它们能够在教育技术、统计分析辅助系统、自动评估统计教育平台以及质量保障的自动化分析工具中发挥作用。
### Conclusion
微调后的LLMs在统计任务中的表现达到了统计学专业学生的水平，证明了它们在教育技术和统计分析辅助系统方面的部署潜力，并且它们能够更准确地评估答案的质量，适用于自动化的统计教育评估和质量保证工具。潜在应用还包括学术和工业研究方法的验证工具，以及数据工作流的质量控制机制。
## 15. `cs.CL` - ClaimDB：大规模结构化数据上的事实验证基准 [PDF](https://arxiv.org/pdf/2601.14698), [HTML](https://arxiv.org/abs/2601.14698)
### Authors
Michael Theologitis,Preetam Prabhu Srikar Dammu,Chirag Shah,Dan Suciu
### Background
尽管事实验证基准取得了显著进展，但基于大规模结构化数据的声明仍然缺乏探索。现有的研究大多关注于简单数据，而本文探讨了包括治理、医疗、媒体、教育和自然科学在内的80个独特的真实生活数据库的基于百万级记录和多表的综合数据集。
### Innovation
引入了名为ClaimDB的新基准，这是第一个证明声明证据来自于数百万记录和多个表组合的事实验证基准。研究发现大多数最新的LLM在准确性和可靠性方面存在显著不足，提出了从阅读证据转向使用可执行程序进行推理的新方法。
### Conclusion
大规模的验证方法在处理这样的数据集时效果不佳。经过广泛实验，即便是最先进的LLM，其准确率也未超过83%，并且大多数模型在处理缺乏证据的情况下难以做出合理决定，这增添了其在高风险数据分析中的可靠性疑问。最终，研究组已经发布了基准测试、代码和LLM排行榜，地址见文末。
## 16. `cs.CL` - Business Logic-Driven Text-to-SQL Data Synthesis for Business Intelligence [PDF](https://arxiv.org/pdf/2601.14518), [HTML](https://arxiv.org/abs/2601.14518)
### Authors
Jinhui Liu,Ximeng Zhang,Yanbo Ai,Zhou Yu
### Background
在私人商业智能(BI)环境中评估文本到SQL代理具有挑战性，因为现实的、领域特定的数据稀缺。现有的生成方法未能捕捉商业现实，即问题是否反映实际的商业逻辑和工作流。生产规模的Salesforce数据库中的实验显示，现有的合成数据集在商业现实捕获方面不如OmniSQL和SQL-Factory，且SQL-Factory表现最差。
### Innovation
提出了一种业务逻辑驱动的数据合成框架，该框架生成的数据基于业务角色、工作场景和工作流程。此外，还提出了业务推理复杂度控制策略，以多样化回答问题所需的分析推理步骤。实验结果表明，合成数据在商业现实方面达到98.44%的高水准，显著优于OmniSQL（+19.5%）和SQL-Factory（+54.7%），同时保持强烈的问题-SQL对齐（98.59%）。合成数据还揭示了最先进的文本到SQL模型在最复杂的商业查询执行准确性上仍然存在显著差距，准确率为42.86%。
### Conclusion
通过业务逻辑驱动的数据合成方法提高了合成数据的商业现实性和问题-SQL对齐程度，且在最复杂查询的表现上仍有较大提升空间。
## 17. `cs.CL` - Say Anything but This: 当 token 化器欺骗 LLM 的推理时 [PDF](https://arxiv.org/pdf/2601.14658), [HTML](https://arxiv.org/abs/2601.14658)
### Authors
Navid Ayoobi,Marcus I Armstrong,Arjun Mukherjee
### Background
大规模语言模型（LLMs）基于离散的 token ID 序列进行推理，然而现代的子词分词器通常会产生非唯一的编码：多个 token ID 序列可以组合成相同的表面字符串。这种表示上的不匹配导致了一种未被测量的脆弱性，会影响到推理过程的稳定性。即使是从语义上看是相同的 token，LLMs 也可能将其视为不同的“单词”。在本研究中，作者揭示了 token 化在特定条件下的欺骗性影响。
### Innovation
作者引入了一个 token 化一致性探测器，要求模型在保持上下文中的所有其他内容不变的情况下替换指定的目标词。此任务在表面上是简单的，使得我们可以将错误归因于分词器-反分词器的缺陷而非知识缺失或参数限制。分析了来自多个最先进的开源 LLM 的超过 11000 次替换试验后，发现输出中存在“幻象编辑”的情况，这种现象是由于分词器引起的表示缺陷所致。作者进一步对这些情况进行分析，并提出了八种系统性分词器缺陷的分类，包括上下文边界调整和词内重新分词。
### Conclusion
研究表明，部分看似推理不足的现象实际上源于分词层。因此，建议在增加模型训练规模之前首先解决分词器层面的问题。
## 18. `cs.CL` - Social Caption：评估多模态模型中的社会理解 [PDF](https://arxiv.org/pdf/2601.14569), [HTML](https://arxiv.org/abs/2601.14569)
### Authors
Bhaavanaa Thumu,Leena Mathur,Youssouf Kebe,Louis-Philippe Morency
### Background
多模态大型语言模型（MLLMs）需要理解人类社会互动的能力才能更好地进行交互。现有的评估方法较为单一，难以全面衡量这些模型的社会理解能力。
### Innovation
文章提出了一种名为Social Caption的框架，通过三个维度（Social Inference，Holistic Social Analysis，Directed Social Analysis）来评估MLLMs的社会理解能力，并分析了影响模型表现的因素，包括规模、架构设计和口语语境。
### Conclusion
通过多模态大型语言模型裁判的实验证明，该方法能够提供关于自动评估多模态社会理解规模化的有价值的见解。
## 19. `cs.CL` - AdaTIR：基于难度感知的策略优化实现适应性工具集成推理 [PDF](https://arxiv.org/pdf/2601.14696), [HTML](https://arxiv.org/abs/2601.14696)
### Authors
Zhaiyu Fang,Ruipeng Sun
### Background
大型语言模型（LLMs）已经通过工具集成推理（TIR）显著增强了能力，但目前的智能代理却倾向于过度依赖外部工具，即使对于简单的任务也会重复调用工具。这对于智能代理来说是认知上的卸载，即在不需要时调用外部工具，降低了效率。
### Innovation
本文提出AdaTIR（Adaptive Tool-Integrated Reasoning）框架，通过引入难度感知效率奖励，使代理智能从静态工具调用转向基于任务难度的内部推理。同时，AdaTIR提出了剪裁优势形成（CAS），解决了工具惩罚超过正确性奖励的反向问题，确保正确性仍然是主要目标，而效率则作为次要约束。实验结果显示，AdaTIR在简单任务中减少了97.6%的工具调用，在复杂任务中减少了28.2%，同时保持或提高了准确性。
### Conclusion
研究证明，AdaTIR在促进推理内部化的同时保持了高效性能，即使在完全禁用工具访问的情况下，AdaTIR也优于基线模型，特别是在AIME 2024测试中提高了4.8%。
## 20. `cs.CL` - SearchGym: 通过经济且高保真环境模拟培育现实世界搜索代理 [PDF](https://arxiv.org/pdf/2601.14615), [HTML](https://arxiv.org/abs/2601.14615)
### Authors
Xichen Zhang,Ziyi He,Yinghao Zhu,Sitong Wu,Shaozuo Yu,Meng Chu,Wenhu Zhang,Haoru Tan,Jiaya Jia
### Background
搜索代理已经成为了解决开放性、知识密集型推理任务的关键范式。然而，通过强化学习（RL）训练这些代理时面临一个关键困境：与实时的商业Web API交互成本极其高昂，而依靠静态数据快照则会导致由于数据不一致而引入噪声，这种不一致生成了破坏性奖励信号，从而稳定了训练过程，错误地惩罚了正确的推理或奖励了虚构的内容。
### Innovation
我们提出了SearchGym，一种模拟环境，旨在培育健壮的搜索代理。SearchGym通过严格的生成管道构建了一个可验证的知识图谱和对齐的文档集合，确保每个推理任务都是基于事实和严格可解的。基于这一可控环境，我们引入了SearchGym-RL，一种课程学习方法，逐步优化代理策略，从基本的交互到复杂的长历规划。广泛的实验跨Llama和Qwen家族表明了强大的模拟到现实世界的泛化能力。特别是在Qwen2.5-7B-Base模型在SearchGym中训练后，相对于网页增强的ASearch者基准，在九个不同基准测试中显示出10.6%的平均相对优势。
### Conclusion
我们的结果证明，高保真模拟为开发高效的搜索代理提供了一种可扩展且成本效益高的方法。
## 21. `cs.SE` - 基于相对合约语言的智能合约形式化验证概念验证 [PDF](https://arxiv.org/pdf/2601.14427), [HTML](https://arxiv.org/abs/2601.14427)
### Authors
Murilo de Souza Neves,Adilson Luiz Bonifacio
### Background
智能合约作为一种具有自我执行能力的工具，相比传统合同提供了增强的安全性，但其不可变性使得故障纠正需要在部署后极度复杂，表明在部署前需要添加一个验证层。尽管合约语言（CL）这样的形式化方法可以进行逻辑分析，但在复杂的多边场景中，它们在归责方面的能力有限。
### Innovation
提出了使用相对合约语言（RCL）和RECALL工具为涉及多个代理的购买和销售合同进行规范和验证的证明概念。该研究展示了该工具在建模阶段发现规范冲突的能力，并且确认了从逻辑不一致翻译为Solidity代码并最终在Remix IDE中功能验证的重要性，即在最终代码可靠性与安全性方面提前形式验证是必要的。
### Conclusion
在智能合约模型化之前进行的正式验证是确保最终代码可靠性和安全性的关键。使用RCL工具能够检测规范冲突，这是一种改进现有合约语言的形式化方法，从而确保智能合约符合预期条件。
## 22. `cs.SE` - GitHub上AI编程代理失败的原因：一项关于失败的代理性拉取请求的实证研究 [PDF](https://arxiv.org/pdf/2601.15195), [HTML](https://arxiv.org/abs/2601.15195)
### Authors
Ramtin Ehsani,Sakshi Pathak,Shriya Rawal,Abdullah Al Mujahid,Mia Mohammad Imran,Preetha Chatterjee
### Background
AI编程代理现在正向软件项目提交拉取请求(PRs)，不仅仅是作为助手，而是作为自主贡献者。这些代理性的贡献正在GitHub等真实仓库中迅速增加，但目前对其在实际中的表现知之甚少，特别是为什么许多PR未能被合并。本文通过对GitHub上由五个编程代理提交的33000个PR进行大规模研究，探讨了这一问题。
### Innovation
研究首次定量表征了合并和不合并的PR，并基于任务类型、代码变更、CI构建结果和审查动态等四个广泛维度进行分析。研究还进一步定性分析了600个未合并的PR，发现了未合并原因的多层次分类，解释了定量指标无法捕捉到的一些问题，如缺乏有意义的评论家参与、重复的PR、不希望的功能实现和代理不对其期望的状况。
### Conclusion
研究结果强调了社会和技术因素以及人类与AI协作的关键因素，这些因素对于提高未来代理性工作流的成功至关重要。通过揭示这些因素，本文为理解AI编程代理在GitHub上的表现提供了重要的见解。
## 23. `cs.SE` - 超越功能正确性：探索Large语言模型生成代码中的幻觉 [PDF](https://arxiv.org/pdf/2404.00971), [HTML](https://arxiv.org/abs/2404.00971)
### Authors
Fang Liu,Yang Liu,Lin Shi,Zhen Yang,Li Zhang,Xiaoli Lian,Zhongqi Li,Yuchi Ma
### Background
大型语言模型（LLMs）在软件工程任务中的应用，特别是在代码生成方面取得了显著进展。尽管这些模型表现出色，但它们容易生成幻觉，即这些模型生成的输出可能与用户意图不符、内部不一致或与现实知识不匹配，这使得在各种应用程序中部署LLMs存在潜在风险。现有的研究主要集中在自然语言生成（NLG）领域对幻觉的调查上，忽视了代码生成背景下的幻觉类型、原因和影响的全面理解。因此，有必要开展更深入的研究来填补这一空白。
### Innovation
本研究通过主题分析LLMs生成的代码，总结并归类了幻觉及其原因和影响，建立了一个全面的代码幻觉分类体系，包含3个主要类别和12个具体类别。通过系统地分析幻觉的分布，揭示了不同LLMs和基准之间幻觉的差异。深入探讨了各种幻觉的原因和影响，提供了幻觉缓解的宝贵见解。进一步探索了通过提示增强技术实现无训练幻觉缓解的方法，以轻量级的方式提升LLMs生成代码的正确性和可靠性。
### Conclusion
本研究为未来的代码幻觉评估和缓解研究奠定了基础，为未来构建更加有效和可靠的代码LLMs铺平了道路。复现包可在以下网址获得：this https URL
## 24. `cs.SE` - UniCon: 一种高效的机器人学习传输统一系统 [PDF](https://arxiv.org/pdf/2601.14617), [HTML](https://arxiv.org/abs/2601.14617)
### Authors
Yunfeng Lin,Li Xu,Yong Yu,Jiangmiao Pang,Weinan Zhang
### Background
部署基于学习的控制器到不同平台的机器人面临挑战，因为不同平台之间的差异、接口不一致和中间件效率低下。这些问题阻碍了机器人控制器的有效迁移。
### Innovation
提出了UniCon，一种轻量级架构，标准化了不同平台上的状态、控制流和仪器。它将工作流分解为执行图，采用可重用组件，并将系统状态与控制逻辑分离，从而实现不同机器人形态间的即插即用部署。与传统的中间件相比，UniCon通过批处理和向量化的数据流优化效率，减少了通信开销并提高了推理延迟。这种模块化和数据导向的方法使得仿真到现实世界的转移可以无缝进行，减少了重新工程的需求。
### Conclusion
UniCon减少代码冗余，实现更高效的推理效率。在12个不同制造商的12种机器人模型上部署，成功集成到了正在进行的研究项目中，证明了其在实际场景中的有效性。
## 25. `cs.SE` - 利用面向对象设计实现知识表示与推理 [PDF](https://arxiv.org/pdf/2601.14840), [HTML](https://arxiv.org/abs/2601.14840)
### Authors
Abdelrhman Bassiouny,Tom Schierenbeck,Sorin Arion,Benjamin Alt,Naren Vasantakumaar,Giang Nguyen,Michael Beetz
### Background
面向对象编程（OOP）是开发复杂应用程序的标准方法，但现有的知识表示与推理（KR&R）框架经常依赖于外部本体和专门的编程语言，这些语言难以与命令式代码集成。因此，存在需求来设计一种新的框架，能够更好地整合现代软件工程与KR&R系统之间的差距。
### Innovation
KRROOD框架通过使用本地类结构将知识作为一级编程抽象来解决上述问题，从而弥合了逻辑编程与面向对象编程范式的差距。该框架在OWL2Bench基准测试和人类-机器人任务学习场景中进行了评估，实验证明了其在支持真实自主系统所需的丰富推理的同时，也能实现强劲的性能。
### Conclusion
KRROOD框架成功地在保留面向对象编程的优势的同时，增强了知识表示与推理的功能，证明了该方法在实际应用中的有效性。
## 26. `cs.SE` - 使用编译器反馈进行迭代改进的ABAP代码生成大型语言模型基准研究 [PDF](https://arxiv.org/pdf/2601.15188), [HTML](https://arxiv.org/abs/2601.15188)
### Authors
Stephan Wallraven,Tim Köhne,Hartmut Westenberger,Andreas Moser
### Background
尽管生成式AI在许多编程语言中的应用已取得成功，但迄今为止对ABAP代码生成的系统性分析非常有限。这项研究旨在通过实证分析来评估各种大型语言模型（LLMs）在生成语法正确且具功能性的ABAP代码方面的表现，以及它们如何利用编译器反馈进行迭代改进，并识别哪些任务类型特别具有挑战性。
### Innovation
本研究通过一个包含180个任务的基准测试，这些任务包括修改后的HumanEval任务以及实际的SAP场景，实证评估了各种LLMs在生成ABAP代码方面的能力。
### Conclusion
研究表明，更强大的LLMs在经过多次迭代后能够实现约75%的成功率，并且从编译器反馈中受益良多；而较小的模型表现显著不佳。总体而言，本研究强调了强大LLMs在ABAP开发过程中的巨大潜力，特别是在迭代错误纠正方面。
## 27. `cs.SE` - AlertGuardian：大规模云系统智能警报生命周期管理 [PDF](https://arxiv.org/pdf/2601.14912), [HTML](https://arxiv.org/abs/2601.14912)
### Authors
Guangba Yu,Genting Mai,Rui Wang,Ruipeng Li,Pengfei Chen,Long Pan,Ruijie Xu
### Background
当前云系统产生的警报数量过多，导致操作效率降低，主要由于缺乏有效的警报生命周期管理。这会对云系统的可靠性和用户体验产生不利影响。
### Innovation
提出了一个名为AlertGuardian的框架，该框架结合了大语言模型和轻量级图模型，分为三个阶段：警报去噪、警报摘要和警报规则优化。具体地，它利用图学习模型和虚拟噪声来过滤噪声警报，通过检索增强生成（RAG）与大语言模型创建可操作的总结，并通过多代理迭代反馈来提高警报规则的质量。
### Conclusion
AlertGuardian在Company-X的四个真实世界数据集上得到有效验证，显著降低了警报疲劳（94.8%的警报减少率）并加快了故障诊断（90.5%的诊断准确率）。此外，该框架还改进了1,174个警报规则，其中375个被SRE接受（接受率32%）。部署后分享了关于警报生命周期管理的成功故事和经验教训。
## 28. `cs.SE` - 当代理失败时：LLM代理中的缺陷的全面研究及其自动标注 [PDF](https://arxiv.org/pdf/2601.15232), [HTML](https://arxiv.org/abs/2601.15232)
### Authors
Niful Islam,Ragib Shahriar Ayon,Deepak George Thomas,Shibbir Ahmed,Mohammad Wardat
### Background
大型语言模型（LLMs）已经彻底改变了智能应用的开发。虽然独立的LLM无法执行任何行为，但LLM代理通过整合工具来解决这一限制。然而，调试LLM代理既困难又昂贵，因为该领域仍处于起步阶段，社区也不够发达。
### Innovation
本文进行了首次全面研究LLM代理程序中的缺陷类型、根本原因及其影响，通过对Stack Overflow、GitHub和Hugging Face论坛上1,187个与缺陷相关的帖子和代码片段进行收集与分析，特别是针对使用七种广泛使用的LLM框架及其自定义实现构建的代理。此外，还研究了缺陷发生的具体组件、编程语言和框架。为了探讨自动化检测缺陷的可能性，还构建了一个名为BugReAct的ReAct代理，配备适当的外部工具，以确定其在数据集中检测和标注缺陷的能力。实验结果显示，BugReAct配备了Gemini 2.5 Flash在标注缺陷特征方面表现出色，平均每张帖子/代码片段的成本为0.01美元。
### Conclusion
该研究发现，BugReAct能够高效地标注缺陷特征，为自动化缺陷检测提供了可行性。
## 29. `cs.SE` - 基于HyperNet的扩散模型测试用例生成自适应方法 [PDF](https://arxiv.org/pdf/2601.15041), [HTML](https://arxiv.org/abs/2601.15041)
### Authors
Oliver Weißl,Vincenzo Riccio,Severin Kacianka,Andrea Stocco
### Background
随着深度学习系统的广泛应用，对其在实际场景下的可靠性进行系统评估变得日益重要。传统的梯度基于对抗攻击方法虽然能够评估模型的鲁棒性，但在实际应用中生成的微小扰动往往不具有代表性，更多地关注模型的鲁棒性而非功能行为。此外，生成测试方法虽然提供了一种替代方案，但在面向大规模测试时往往局限于简单数据集或输入域限制。扩散模型能够实现高质量的图像合成，但其高昂的计算成本和有限的可控性限制了其在大规模测试中的应用。
### Innovation
本文提出了一种名为HyNeA的生成测试方法，它通过使用超网络实现直接且高效的扩散生成控制。HyNeA能够在不依赖特定架构的条件机制和依赖数据的微调的情况下，提供无数据集依赖的可控性。它通过实例级别调节来识别可能导致失败的测试用例，无需使用包含类似失败示例的数据集进行微调。这种方法能够以低于基于搜索方法的计算成本生成更真实且有效的失败测试用例。
### Conclusion
实验结果表明，HyNeA相比现有的生成测试生成器，提高了可控性和测试多样性，并且能适应那些没有标注失败训练数据的领域。
## 30. `cs.SE` - Protocode: Prototype-驱动代码生成中的可解释性 [PDF](https://arxiv.org/pdf/2509.25247), [HTML](https://arxiv.org/abs/2509.25247)
### Authors
Krishna Vamshi Bodla,Haizhao Yang
### Background
自大型语言模型（LLMs）出现以来，它们被广泛应用于文本摘要、问答、语音到文本的翻译等多种任务中。近期，LLMs 在代码生成方面的应用引起了广泛关注，例如 Cursor 和 Windsurf 这类工具能够分析大规模代码库并推荐相关改动。大科技公司意识到 LLMs 在代码生成方面的依赖日益增加，尽管这些进步极大地提高了开发者的生产力，但过度依赖自动化代码生成可能会增加产生次优解决方案和安全代码风险。本文的研究重心在于自动采样 In-Context Learning (ICL) 举证，这些举证可以提升模型性能并增强生成代码的可解释性。通过使用基于 AST 的分析对 MBPP 测试集的输出进行研究，我们识别出哪些代码区域最受到所选举证的影响。实验结果表明，高质量的 ICL 举证不仅使生成的代码更易于理解，还能够在 pass@10 指标上取得积极的性能提升，而差的 ICL 举证则会降低 LLM 的性能。
### Innovation
本文介绍了一种自动采样 In-Context Learning (ICL) 举证的方法，该方法能够提高模型性能并增强代码生成的可解释性。研究人员通过 MBPP 测试集的 AST 分析工具，确定了代码生成最依赖的区域。实验证明，高质量的 ICL 举证不仅有助于提高代码的解释性，还能改善 pass@10 指标。
### Conclusion
本文的研究突显了高效 ICL 采样策略的重要性，这些策略能够影响模型在任何给定任务上的性能。使用高质量 ICL 举证不仅提升了代码生成的质量，还增强了其可解释性。
