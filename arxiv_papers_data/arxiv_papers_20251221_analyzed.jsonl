{"llm_update_time": "20251221", "topic": "cs.AI", "pdf_url": "https://arxiv.org/pdf/2512.15776", "html_url": "https://arxiv.org/abs/2512.15776", "title": "Emergence: 通过主动查询克服拥有信息偏差在不对称实体代理中的障碍", "title_en": "Emergence: Overcoming Privileged Information Bias in Asymmetric Embodied Agents via Active Querying", "authors": "Shaun Baek,Sam Liu,Joseph Ukpong", "background": "大语言模型（LLMs）作为强大的推理引擎，在具身环境中表现出“符号接地”困难，尤其是当信息不对称时。本文调查了“已知信息偏差”（或“报童现象”），即由于缺乏心智理论，知识丰富的“领导者”无法有效地引导感官受限的“跟随者”。", "innovation": "本文提出了一种新颖的不对称辅助推理框架，该框架在AI2-THOR中量化了这种现象。研究发现，“拉式”协议（主动查询）相比传统的“推式”指令更为 robust，成功会话中澄清请求的频率几乎翻倍。", "conclusion": "研究隔离了减少主动不确定性的机制作为安全的人机及机器人协作的前提。"}
{"llm_update_time": "20251221", "topic": "cs.AI", "pdf_url": "https://arxiv.org/pdf/2512.15824", "html_url": "https://arxiv.org/abs/2512.15824", "title": "基于状态增强图的循环经济分类方法", "title_en": "State-Augmented Graphs for Circular Economy Triage", "authors": "Richard Fox,Rui Li,Gustav Jonsson,Farzaneh Goli,Miying Yang,Emel Aktas,Yongjing Wang", "background": "在产品达到使用寿命后，如何通过循环经济模式进行处理和再利用是一个评估其可持续路径的问题。有效的循环分类决策需要在保留价值和处理成本及劳动力限制之间进行适应性平衡。", "innovation": "本文提出了一种新型决策框架，作为状态增强的拆卸顺序规划图的简单确定性求解器。通过将拆卸历史编码到状态中，该框架采用马尔科夫性质，通过确保每个决策仅依赖于前一状态来实现最优递归评估。该框架在电动车电池的多层次分类实例中得到演示，展示了其灵活性。", "conclusion": "该框架提供了一种可用于跨不同产品和操作场景优化循环经济分类决策的统一且通用的基础。"}
{"llm_update_time": "20251221", "topic": "cs.AI", "pdf_url": "https://arxiv.org/pdf/2512.15736", "html_url": "https://arxiv.org/abs/2512.15736", "title": "Anubuddhi: 一种用于设计和模拟量子光学实验的多智能体AI系统", "title_en": "Anubuddhi: A Multi-Agent AI System for Designing and Simulating Quantum Optics Experiments", "authors": "S. K. Rithvik", "background": "量子光学实验的设计依赖于特定领域知识，通常需要专业知识和编程技能。现有的设计系统往往缺乏对自然语言指令的处理能力，限制了其在非专业人士中的应用。", "innovation": "提出了Anubuddhi，一个无需编程知识的多智能体AI系统，能够从自然语言指令中设计和模拟量子光学实验。该系统通过语义检索组件、物理模拟验证构建量子光学布局，并采用QuTiP和FreeSim双重验证机制。", "conclusion": "实验表明，Anubuddhi 在13个不同类型的量子光学实验中实现了设计与模拟的一致性评价得分（8-9/10），提供了更加灵活强大的数学表示，并能有效促进科研和教学中的计算实验设计。此外，该研究强调了结构正确性与数值准确性之间的区别，并指出自由形式的模拟框架优于约束框架。"}
{"llm_update_time": "20251221", "topic": "cs.AI", "pdf_url": "https://arxiv.org/pdf/2512.15784", "html_url": "https://arxiv.org/abs/2512.15784", "title": "超越训练：通过MOBIMEM实现代理的自我演变", "title_en": "Beyond Training: Enabling Self-Evolution of Agents with MOBIMEM", "authors": "Zibin Liu,Cheng Zhang,Xi Zhao,Yunfei Feng,Bingyu Bai,Dahu Feng,Erhu Feng,Yubin Xia,Haibo Chen", "background": "大型语言模型（LLM）代理在移动和桌面环境中被越来越多地部署以自动化复杂的工作流程。然而，现有的基于模型的代理架构在部署后难以自我进化：改善个性化、能力和效率通常需要不断对模型进行重新训练/精细调整，这产生了巨大的计算开销，并存在模型准确性与推理效率之间的固有权衡。", "innovation": "为了在无需重新训练模型的情况下实现迭代自我进化，我们提出了一种基于内存的代理系统（MOBIMEM）。MOBIMEM引入了三种专门的内存原语，将代理进化与模型权重解耦：（1）配置内存使用轻量级距离图（DisGraph）结构来与用户偏好对齐，解决了用户配置检索的准确性和延迟之间的权衡；（2）经验内存使用多级模板来为新任务实例化执行逻辑，确保能力泛化；（3）行动内存记录了细粒度的交互序列，减少了对昂贵模型推理的依赖。在此内存架构的基础上，MOBIMEM进一步整合了一系列类操作系统的服务来协调执行：调度器协调并行子任务执行和内存操作；代理记录和重放（AgentRR）机制允许安全有效地重用行动；以及上下文感知的异常处理，确保在用户中断或运行时错误时的优雅恢复。", "conclusion": "在AndroidWorld和顶级50个应用上的评估表明，MOBIMEM在用户配置检索方面实现了83.1%的对齐度，检索时间为23.83毫秒（比GraphRAG基准快280倍），将任务成功率提高高达50.3%，并在移动设备上将端到端延迟减少高达9倍。"}
{"llm_update_time": "20251221", "topic": "cs.AI", "pdf_url": "https://arxiv.org/pdf/2512.15922", "html_url": "https://arxiv.org/abs/2512.15922", "title": "利用传播激活提高基于知识图谱的检索增强生成系统中的文档检索", "title_en": "Leveraging Spreading Activation for Improved Document Retrieval in Knowledge-Graph-Based RAG Systems", "authors": "Jovan Pavlović,Miklós Krész,László Hajdu", "background": "尽管检索增强生成（RAG）系统在初始阶段取得了成功，并且具有了多种架构，但它们仍然难以可靠地检索和连接多步骤证明所需的信息，这对于复杂的推理任务至关重要。当前的标准RAG框架会将所有检索到的信息视为同等可信，忽略了大型文本语料库中的透明性和相互关联性。图RAG方法通过引入知识图谱来改善这一状况，知识图谱将信息组织成节点和边，捕捉实体关系并允许多步骤逻辑遍历。然而，图RAG方法并非总是最佳选择，因为它们依赖高质量的语料库图表示，这要么需要昂贵且耗时的预先存在的知识图谱，要么依赖于通常不可靠的自动化图构建管道。此外，遵循该范式的系统通常使用大型语言模型来引导图遍历和证据检索，这带来了与标准RAG类似的挑战。", "innovation": "本研究提出了一种新的RAG框架，利用传播激活算法从文档中获取信息，这些文档通过自动构建的知识图谱相互连接，从而增强大型语言模型在多跳问答等复杂任务上的性能。与迭代RAG方法相比，我们的方法在实验中表现出更好的或可比的表现，并且可以轻松作为 plug-and-play 模块接入多种基于RAG的方法。并且，我们的方法结合链式思考迭代检索可以实现39%绝对增加的答案准确性，使用小型开放权重语言模型，突显了其在资源受限环境中的效果。", "conclusion": "我们的研究表明，我们提出的方法不仅在复杂的任务表现良好，而且可以轻松集成到各种RAG系统中，同时展示了其在资源限制环境中的有效性。结合链式思考迭代检索的方法提高解答正确率高达39%，使用小型语言模型，这更加凸显了其灵活性和效率。"}
{"llm_update_time": "20251221", "topic": "cs.AI", "pdf_url": "https://arxiv.org/pdf/2512.15783", "html_url": "https://arxiv.org/abs/2512.15783", "title": "AI流行病学：通过专家监督模式实现可解释的人工智能", "title_en": "AI Epidemiology: achieving explainable AI through expert oversight patterns", "authors": "Kit Tempest-Walters", "background": "当前可解释性方法（如SHAP和机制解释方法）在大规模部署模型中存在模型复杂性的问题。AI流行病学通过应用公共卫生干预中的整体监控框架，以提高高级AI系统的透明度和可解释性。", "innovation": "提出了一种新颖的框架，通过标准化AI专家交互记录为结构化的评估字段（风险级别、一致性评分、准确度评分），并利用统计关联验证输出失败，以实现可解释AI。该框架自动记录专家对AI建议的同意或分歧，无需依赖于复杂的模型内部计算，并确保机构更新模型或更换供应商时的治理连续性。此外，通过提供可靠性和语义评估，它使专家能够在AI输出可能导致危害之前发现不可靠的AI输出。", "conclusion": "通过提供可靠的和语义化的评估，AI流行病学使领域专家能够在没有机器学习专业知识的情况下管理AI系统，从而实现了AI的民主化监管。"}
{"llm_update_time": "20251221", "topic": "cs.AI", "pdf_url": "https://arxiv.org/pdf/2512.15743", "html_url": "https://arxiv.org/abs/2512.15743", "title": "Prompt-to-Parts：生成式AI在物理组装和可扩展指令中的应用", "title_en": "Prompt-to-Parts: Generative AI for Physical Assembly and Scalable Instructions", "authors": "David Noever", "background": "以往的研究大多侧重于从自然语言生成不受限制的3D文本，缺乏对几何有效性、连接约束和可建造性顺序的强制执行。传统的像素扩散方法或计算机辅助设计(CAD)模型在支持复杂组装指令或组件交换方面存在不足。本文提出了一个从自然语言生成实际可装配产品的装配指令框架。", "innovation": "本文提出了一种新的基于BRICK的方法，该方法在离散零件词汇表中运作，通过LDraw作为丰富的文本中间表示，指导大型语言模型生成有效且逐步的组装序列。同时，本文还引入了一个Python库用于程序生成模型，并在复杂卫星、飞机和建筑领域评估了构建输出，填补了之前像素扩散方法或CAD模型在支持复杂组装指令或组件交换方面的能力空白。", "conclusion": "该方法旨在实现可验证的扩展性、模块化和精度，从而弥合语义设计意图与可制造输出之间的差距。这种物理原型在自然语言规范的基础上生成。该研究提出了一种新颖“砖块集合”的方法，作为一种关键缺失组件，填补了传统像素扩散方法或CAD模型在支持复杂组装指令或组件交换方面的空白，使新的设计选项成为可能，并引导制造业和工程原型中的自然语言实现。"}
{"llm_update_time": "20251221", "topic": "cs.AI", "pdf_url": "https://arxiv.org/pdf/2512.15740", "html_url": "https://arxiv.org/abs/2512.15740", "title": "比例责任原则：人类与人工智能系统中道德平衡的知识责任框架", "title_en": "The Principle of Proportional Duty: A Knowledge-Duty Framework for Ethical Equilibrium in Human and Artificial Systems", "authors": "Timothy Prescher", "background": "传统伦理框架在处理不确定性下的决策时往往存在局限性，往往将其视为行动的简单约束条件。本文引入了‘比例责任原则’（PPD），这是一种创新框架，它展示了随着代理人的知识状态的变化，道德责任如何相应地扩大或缩小。", "innovation": "文章提出了比例责任原则（PPD），这是一种新的方法来量化伦理责任如何根据代理人的知识状态进行调整。这种动态表示为公式 D_total = K[(1-HI) + HI * g(C_signal)]，其中Total Duty 是知识（K）、谦逊/不确定性（HI）和情境信号强度（C_signal）的函数。文章通过蒙特卡洛模拟证明，保持最低谦逊系数的系统能够更稳定地分配责任，减少过度自信决策的风险。", "conclusion": "通过将谦逊作为系统参数正式化，比例责任原则为道德责任提供了一种数学上可处理的方法，这可能有助于指导审计可问责的AI决策系统的研发。文章跨四个领域（临床伦理、接受者权利法、经济治理和人工智能），展示了其跨学科的有效性。研究结果表明，比例责任原则在复杂系统中是一个稳定原则，可以动态平衡知识自信和情境风险，防止过度干预和疏漏。"}
{"llm_update_time": "20251221", "topic": "cs.AI", "pdf_url": "https://arxiv.org/pdf/2512.15906", "html_url": "https://arxiv.org/abs/2512.15906", "title": "达斯·vec多尔：一种通过大规模语言模型查询生成知识图谱的开源系统", "title_en": "Darth Vecdor: An Open-Source System for Generating Knowledge Graphs Through Large Language Model Queries", "authors": "Jonathan A. Handler", "background": "许多大规模语言模型（LLMs）是在互联网上大量的知识基础上进行训练的。达斯·vec多尔（DV）被设计用于从这些知识中提取出结构化的术语映射的SQL数据库（知识库或知识图谱）。虽然可以直接查询LLM而非基于SQL的知识图谱，但在高工作量操作中可能会出现成本、速度、安全性和信心方面的问题。通过将LLM中的信息预先提取出来并以标准数据库形式进行查询，这些问题可以被缓解。然而，作者发现需要解决几个问题，这些问题包括LLM响应的错误、无关、自由文本、过于宽泛和不一致以及多元素响应。", "innovation": "DV通过具备的功能解决这些问题，包括错误、无关、自由文本、过于宽泛和不一致的LLM响应。DV提供了一个简单的、基于浏览器的图形用户界面，便于非技术背景但有专业领域知识的人完成提示工程。DV作为开源、可扩展的软件发布，用户需自行承担风险并确保使用安全有效。", "conclusion": "尽管DV可能存在一些现有的或未来的严重错误，但作者希望合理使用当前及未来的DV及其产出能有助于改善医疗健康状况。用户需要认识到DV及其输出的潜在风险和利益，并确保使用时的安全和有效性。"}
{"llm_update_time": "20251221", "topic": "cs.AI", "pdf_url": "https://arxiv.org/pdf/2512.15894", "html_url": "https://arxiv.org/abs/2512.15894", "title": "PediatricAnxietyBench：在儿科咨询中评估父母焦虑和压力下大型语言模型的安全性", "title_en": "PediatricAnxietyBench: Evaluating Large Language Model Safety Under Parental Anxiety and Pressure in Pediatric Consultations", "authors": "Vahideh Zolfaghari", "background": "随着大型语言模型（LLMs）在父母育儿指导中的使用增加，人们对它们在真实世界对抗性压力下的安全性知之甚少。焦虑的父母经常使用紧急语言，这可能削弱模型的安全措施，导致有害的建议。", "innovation": "本研究提出了一个名为PediatricAnxietyBench的开源基准，包含300个高质量的查询，涵盖了10个儿科主题（150个病人衍生查询，150个对抗性查询），用于重复测试。研究使用多维度安全框架评估了两个Llama模型（70B和8B）的安全性，覆盖诊断抑制、转诊遵守、婉拒和紧急识别。引入了家长压力模式，包括紧迫性、经济障碍和免责声明的挑战。", "conclusion": "大型语言模型在家长焦虑和压力下的安全评分平均为5.50/15（标准差=2.41）。70B模型的性能优于8B模型，且具有较少的关键失败。对抗性查询降低了安全性8%（p=0.03），其中紧迫性影响最大。在癫痫和疫苗接种后的查询中发现了漏洞。以婉拒强烈相关于安全性（r=0.68，p<0.001），而紧急识别的缺乏则是一个明显的问题。模型的规模影响安全性，但所有模型在现实的家长压力下都显示出漏洞。PediatricAnxietyBench提供了一个可重用的对抗性评估框架，用于揭示标准基准忽视的临床相关失败模式。"}
{"llm_update_time": "20251221", "topic": "cs.CL", "pdf_url": "https://arxiv.org/pdf/2512.15792", "html_url": "https://arxiv.org/abs/2512.15792", "title": "大型语言模型偏见系统的分析", "title_en": "A Systematic Analysis of Biases in Large Language Models", "authors": "Xulang Zhang,Rui Mao,Erik Cambria", "background": "大型语言模型（LLMs）已经成为获取信息和辅助人类决策的重要工具，但确保这些模型在各种情境中保持公平对于其安全和负责任的部署至关重要。本研究通过对四种广泛采用的LLMs进行全面考察，探究它们在政治、意识形态、联盟、语言和性别等维度上的潜在偏见和倾向。", "innovation": "该研究设计了一系列精心策划的实验，分别从新闻摘要的政治中立性、新闻立场分类的意识形态偏见、通过联合国投票模式揭示的具体地缘政治联盟倾向、多种语言的故事生成中的语言偏见以及回应世界价值观调查中存在的性别偏好等多个维度系统地分析了大型语言模型的偏见。", "conclusion": "尽管这些LLMs试图保持中立和公正，但它们仍然显示出不同类型的各种偏见和偏好。"}
{"llm_update_time": "20251221", "topic": "cs.CL", "pdf_url": "https://arxiv.org/pdf/2512.15973", "html_url": "https://arxiv.org/abs/2512.15973", "title": "动态强化学习在大语言模型中自适应低秩多头自我注意", "title_en": "Dynamic Rank Reinforcement Learning for Adaptive Low-Rank Multi-Head Self Attention in Large Language Models", "authors": "Caner Erden", "background": "传统的低秩近似往往依赖于静态的秩假设，这限制了其在多种输入上下文中的灵活性。而DR-RL提出了一种新的框架，通过强化学习和在线矩阵扰动理论的结合，动态优化大语言模型中多头自我注意的低秩因子化。", "innovation": "DR-RL的核心创新在于使用一个RL代理将秩选择问题建模为顺序策略优化问题，并通过在线矩阵扰动边界实现增量秩更新，避免在线推理时的完全分解带来的高昂成本。此外，集成了轻量级的Transformer策略网络以及批量SVD操作，确保在现代GPU架构下的可扩展部署。", "conclusion": "实验表明，DR-RL在保持与全秩注意力相统计等效的下游准确性的同时，显著减少了浮点运算（特别在长序列条件下，L > 4096）。这项工作弥合了自适应效率与理论严谨性之间的差距，在资源受限的深度学习中提供了一种经过原理证明的替代方法。"}
{"llm_update_time": "20251221", "topic": "cs.CL", "pdf_url": "https://arxiv.org/pdf/2512.15747", "html_url": "https://arxiv.org/abs/2512.15747", "title": "D3G: 多模态模型中多样化的 demographic 数据生成提高零样本图像分类性能", "title_en": "D3G: Diverse Demographic Data Generation Increases Zero-Shot Image Classification Accuracy within Multimodal Models", "authors": "Javon Hickmon", "background": "图像分类任务是实现人类级图像理解的机器感知任务的核心。通过跨视觉和语言学习语义相似性，像CLIP这样的多模态模型已经在该领域表现出色。然而，图像分类仍然是一项具有挑战性的任务。尤其是在小容量模型中，由于过拟合不足，它们在细粒度图像分类上的表现不佳。此外，生成与各个类别丰富的跨模态表示的数据质量至关重要，而这往往是难以实现的。当数据集没有平衡多样性时，预测倾向于更常见类别的群体，而其他群体则被忽视。因此，作者关注这些问题如何导致零样本图像分类中的有害偏差，并探讨如何在群体偏差方面应对这些问题。", "innovation": "作者提出了一种名为D3G的方法，这是一种无需训练、提供零样本图像分类准确性和降低多模态模型中的人口偏差的预测方法。这种方法利用CLIP作为基础多模态模型，Stable Diffusion XL作为生成模型。通过在推理时提供多样化的群体数据可以改善这些模型的性能，并探讨了个体群体对结果准确度指标的影响。", "conclusion": "实验结果表明，提供多样化的群体数据可以提高这些模型的性能，而不会增加训练成本。这种方法强调了在多模态模型中提高零样本图像分类性能的同时减少群体偏差的重要性。"}
{"llm_update_time": "20251221", "topic": "cs.CL", "pdf_url": "https://arxiv.org/pdf/2512.15782", "html_url": "https://arxiv.org/abs/2512.15782", "title": "自动调优黑盒大型语言模型的安全护栏", "title_en": "Auto-Tuning Safety Guardrails for Black-Box Large Language Models", "authors": "Perry Abdulkadir", "background": "随着大型语言模型（LLMs）在安全性护栏如系统提示和内容过滤器保护下被广泛部署，尤其是在产品团队无法修改模型权重的情况下，这些护栏通常需要手动调整，容易出错且难以复制。因此，研究如何通过自动优化方式改进这一问题变得尤为重要。", "innovation": "本文提出了一种针对黑盒LLM的安全护栏设计方法：将安全护栏设计视为对固定基础模型的超参数优化问题。通过使用Mistral-7B-Instruct加上模块化劫持和恶意软件系统提示，并结合基于ModernBERT的有害性分类器，评估候选配置在三个公开基准上的性能。研究采用了网格搜索和黑盒Optuna优化方法，显示后者不仅减少了评估次数，而且节省了大量计算时间。", "conclusion": "该研究表明，将安全护栏视为可调超参数的方法在计算和时间资源有限的情况下有可能增强黑盒LLM的安全性。而通过使用黑盒优化方法，能够在更少的计算资源下找到最佳的安全护栏配置。"}
{"llm_update_time": "20251221", "topic": "cs.CL", "pdf_url": "https://arxiv.org/pdf/2512.15791", "html_url": "https://arxiv.org/abs/2512.15791", "title": "语言模型中AI伦理工具的评估：开发者的视角案例研究", "title_en": "Evaluation of AI Ethics Tools in Language Models: A Developers' Perspective Case Stud", "authors": "Jhessica Silva,Diego A. B. Moreira,Gabriel O. dos Santos,Alef Ferreira,Helena Maia,Sandra Avila,Helio Pedrini", "background": "随着人工智能（AI）中的语言模型在通过文本生成进行人类模拟对话的系统中的广泛应用，开发和部署这些模型必须负责任地进行，以考虑到它们可能产生的负面影响和潜在危害。鉴于此，最近关于AI伦理工具（AIETs）的出版物有所增加，旨在通过提供公认的价值观来指导AI的设计、开发和使用过程。然而，许多AIETs缺乏良好的文档、使用示例及其实际有效性证明。", "innovation": "本文提出了一种评估语言模型中AI伦理工具的方法。该方法包括对213篇AI伦理工具的广泛文献回顾，并最终选择Model Cards、ALTAI、FactSheets和Harms Modeling等四种AI伦理工具。通过与开发人员进行35小时的访谈，评估这些工具在帮助开发人员识别模型的伦理考虑方面的使用和质量。结果表明，这些AI伦理工具可以作为指导，促进形成一般性的语言模型伦理考虑。但是，它们未能解决这些模型特有的问题，如惯用表达。这些AI伦理工具未能识别出针对葡语语言模型可能产生的负面影响。", "conclusion": "尽管选择的AI伦理工具在一定程度上帮助了识别语言模型的伦理考虑，但它们并未完全覆盖每种语言模型的独特方面。因此，需要进一步优化AI伦理工具，使其更全面地覆盖各种模型的具体情况，特别是在跨语言应用中。"}
{"llm_update_time": "20251221", "topic": "cs.CL", "pdf_url": "https://arxiv.org/pdf/2512.15830", "html_url": "https://arxiv.org/abs/2512.15830", "title": "从分钟到天：通过监督预训练扩展颅内语音解码的规模", "title_en": "From Minutes to Days: Scaling Intracranial Speech Decoding with Supervised Pretraining", "authors": "Linnea Evanson,Mingfang Zhang,Hubert Banville,Saarang Panchavati,Pierre Bourdillon,Jean-Rémi King", "background": "传统的从脑活动解码语音的方法依赖于有限的大脑记录，这些记录是在时间短且高度受控的实验中收集的。这项研究通过引入使用患者在临床监测期间进行的长达一周的颅内及音频记录，将训练数据集的大小增加了两数量级以上，为解码和建模提供了新的路径。", "innovation": "研究提出了一种框架，利用长达一周的颅内及音频记录，通过监督预训练的方法，显著提高了模型性能，特别是针对脑活动数据的扩展规模，模型性能随数据集规模呈对数线性增长。此外，研究还发现了大脑活动的全局结构在不同天数间存在大的漂移，表明需要在模型中明确定义跨天变异性。", "conclusion": "通过这种方法，该项研究为在实际生活和受控任务设置下解码和建模大脑表示提供了一条可扩展的路径。"}
{"llm_update_time": "20251221", "topic": "cs.CL", "pdf_url": "https://arxiv.org/pdf/2512.15798", "html_url": "https://arxiv.org/abs/2512.15798", "title": "DP-Bench: 一个评估数据产品创建系统的基准", "title_en": "DP-Bench: A Benchmark for Evaluating Data Product Creation Systems", "authors": "Faisal Chowdhury,Sola Shirai,Sarthak Dash,Nandana Mihindukulasooriya,Horst Samulowitz", "background": "数据产品是一种旨在解决特定问题、应对特定商业应用场景或满足特定需求的产品，远不止数据作为一种原始资产提供服务那么简单。通过数据产品，终端用户能够获得其数据的更深刻见解。自从十多年前首次提出以来，特别是在工业领域，已经开展了大量的工作来手动或半自动地创建数据产品。然而，目前几乎没有任何方法来评估自动数据产品创建。因此，作者提出一个第一类的评估自动数据产品创建任务的基准，将其命名为DP-Bench。", "innovation": "作者创造了DP-Bench，这是第一个用于评估自动数据产品创建任务的基准。通过借鉴ELT（抽取-加载-转换）和文本到SQL基准的工作，作者描述了基准是如何创建的。同时，作者还提出了几种基于LLM（大型语言模型）的方法，作为自动生成数据产品的基线。", "conclusion": "作者已经将DP-Bench和补充资料发布在特定的网址上，以便研究者和开发者可以使用这个基准来评估各种数据产品创建系统。"}
{"llm_update_time": "20251221", "topic": "cs.CL", "pdf_url": "https://arxiv.org/pdf/2512.15793", "html_url": "https://arxiv.org/abs/2512.15793", "title": "通过生成冲突的社会规范进行可解释的人类行为伦理评估", "title_en": "Explainable Ethical Assessment on Human Behaviors by Generating Conflicting Social Norms", "authors": "Yuxi Sun,Wei Gao,Hongzhan Lin,Jing Ma,Wenxuan Zhang", "background": "人类行为通常受到社会规范的引导或限制，社会规范是指共享的常识规则。现有的依赖大规模数据训练而不基于明确规范来评估人类行为的正面或反对态度（即支持或反对）的AI系统难以解释，因此不具可信度。考虑社会规范来模仿人类评估者可以帮助AI模型更好地理解和预测人类行为的价值。然而，多种规范可能会相互冲突，从而影响人类行为。例如，在决定是否报告目击的犯罪时，人们可能会权衡勇敢与保护自己之间的平衡。", "innovation": "本文介绍了一种名为ClarityEthic的新型伦理评估方法，旨在通过生成支持人类行为的冲突社会规范来增强价值预测和解释，从而通过对比学习策略加强语言模型的道德推理能力。实验结果显示，本方法优于强大的基线方法，人类评估也证明了生成的社会规范为人类行为的评估提供了合理的解释。", "conclusion": "通过生成冲突的社会规范，ClarityEthic方法可以提升AI系统的道德推理能力和价值预测的解释度，增强系统的可信度。"}
{"llm_update_time": "20251221", "topic": "cs.CL", "pdf_url": "https://arxiv.org/pdf/2512.15926", "html_url": "https://arxiv.org/abs/2512.15926", "title": "DSO: 直接引导优化（Direct Steering Optimization）用于偏见缓解", "title_en": "DSO: Direct Steering Optimization for Bias Mitigation", "authors": "Lucas Monteiro Paes,Nivedha Sivakumar,Yinong Oliver Wang,Masha Fedzechkina Donaldson,Luca Zappella,Nicholas Apostoloff", "background": "生成模型经常被部署用于代表用户做出决策，如视觉语言模型（VLMs）识别房间中谁是医生以帮助视障人士。然而，VLM 的决策受到输入中人物所感知的种族属性的影响，可能导致偏差如忽视女性医生。当减少偏见导致性能下降时，用户可能需要在偏见缓解与总体模型能力之间找到平衡，这强调了能够控制偏差减缓的方法的需求。激活引导是一种流行的在推理时控制的方法，在大型语言模型（LLMs）上显示出潜在的安全行为诱导能力。然而，当前的引导方法难以纠正偏见，需要在不同种族群体中实现等概率结果。为此，我们提出了直接引导优化（DSO），它利用强化学习来寻找线性变换，以减轻偏见并保持对模型性能的控制。", "innovation": "DSO 使用强化学习来找到能够减轻偏见同时保持对模型性能控制的线性变换，解决了现有引导方法难以纠正偏见的问题，实现了虚拟语言模型（VLMs）和大型语言模型（LLMs）上公平性和能力之间的最佳权衡，并为实践者提供了在推理时控制这种权衡的能力。", "conclusion": "我们的工作强调了设计直接优化引导策略的好处，这些策略可以直接控制模型行为，提供比依赖预先定义的控制启发式更具效的偏见干预。"}
{"llm_update_time": "20251221", "topic": "cs.CL", "pdf_url": "https://arxiv.org/pdf/2512.15885", "html_url": "https://arxiv.org/abs/2512.15885", "title": "超越文字的视觉理解：多模态大型语言模型的自监督视觉学习", "title_en": "Seeing Beyond Words: Self-Supervised Visual Learning for Multimodal Large Language Models", "authors": "Davide Caffagni,Sara Sarto,Marcella Cornia,Lorenzo Baraldi,Pier Luigi Dovesi,Shaghayegh Roohi,Mark Granroth-Wilding,Rita Cucchiara", "background": "近年来，多模态大型语言模型（MLLMs）在连接视觉和语言方面展现出了令人印象深刻的性能，但在基本的视觉推理任务上表现仍有限。这种局限性源于MLLMs主要通过文本描述学习视觉理解，这些描述构成的监督信号主观且不完整。此外，与大规模单模态预训练相比，多模态指令调整的规模较小，这会导致MLLMs过度适应语言先验，忽略视觉细节。", "innovation": "本文介绍了一种名为JARVIS的JEPA启发式的框架，用于自监督视觉增强MLLMs。具体来说，我们将I-JEPA学习范式集成到MLLMs训练中的标准视觉-语言对齐管道中。该方法利用冻结的视觉基础模型作为上下文和目标编码器，同时通过早期层的大型语言模型（LLM）训练预测器，使其能够从图片中学习结构和语义连贯性，而不需要完全依赖语言监督。", "conclusion": "在标准多模态大语言模型基准上的广泛实验表明，JARVIS能够在不同LLM家族的视觉基准测试中持续提高性能，而不损害多模态推理能力。源代码可从本链接公开获取：this https URL。"}
