# 20260119
[![Subscribe_Visitors](https://visitor-badge.laobi.icu/badge?page_id=nituchao.latest_arxiv_analyze_ai_rss)](https://github.com/nituchao/latest_arxiv_analyze_ai)

## 1. `cs.AI` - PCN-Rec: 具有证明携带谈判功能的代理可靠政策约束推荐系统 [PDF](https://arxiv.org/pdf/2601.09771), [HTML](https://arxiv.org/abs/2601.09771)
### Authors
Aradhya Dixit,Shreem Dixit
### Background
现代基于大规模语言模型（LLM）的推荐器可以生成引人注目的排序列表，但在确保最小长尾曝光或多样性等治理约束方面表现不佳。
### Innovation
提出了一个称为 PCN-Rec 的证明携带谈判管道，它将自然语言推理与确定性执行分离。该管道包括一个基础推荐器（MF/CF）、一个用户倡导者、一个政策代理、一个 mediator LLM 和一个确定性验证器，共同工作以生成符合要求的推荐列表，并提供可审计的审计跟踪。
### Conclusion
在 MovieLens-100K 数据集上应用 PCN-Rec，在满足治理约束的用户中实现了98.55%的成功率，同时保持了与未验证/修复的一次性单LLM基线相比0.021的绝对微小NDCG@10下降(0.403 vs. 0.424)；差异具有统计学意义（p < 0.05）。
## 2. `cs.AI` - 超越基于规则的工作流：CORAL 基于代理到代理通信的信息流协调多代理范式 [PDF](https://arxiv.org/pdf/2601.09883), [HTML](https://arxiv.org/abs/2601.09883)
### Authors
Xinxing Ren,Quagmire Zang,Caelum Forder,Suman Deb,Ahsen Tahir,Roman J. Georgio,Peter Carroll,Zekun Guo
### Background
大多数基于大型语言模型（LLM）的多代理系统（MAS）依赖预定义的工作流程，人类工程师需要预先列举任务状态并指定相应的路由规则和上下文注入。这种基于工作流程的设计本质上是基于规则的决策树，存在两个根本局限：需要大量的手动努力来预测和编码所有可能的任务状态，且无法完全覆盖复杂现实任务的状态空间。
### Innovation
提出了一种基于CORAL的代理到代理（A2A）通信的信息流协调多代理范式，该范式中包含一个专用的信息流协调器，可以持续监控任务进度，并通过A2A工具包使用自然语言动态协调其他代理，而无需依赖预定义的工作流程。这种方法在GAIA通用基准测试中表现出色，相较于基于工作流程的MAS OWL，准确率提高了8.49个百分点。
### Conclusion
该范式能够实现更灵活的任务监控，并更好地处理边缘情况。文章提供的实现已经公开，可以在指定的网址中获取。
## 3. `cs.AI` - 大语言模型用户中的反社会行为：实验证据 [PDF](https://arxiv.org/pdf/2601.09772), [HTML](https://arxiv.org/abs/2601.09772)
### Authors
Paweł Niszczota,Cassandra Grützner
### Background
大语言模型的迅速普及引发了对它们引起的社会反应的担忧。先前的研究记录了AI用户持有的负面态度，但尚不清楚这种不赞成是否会导致实际的惩罚性行为。本研究通过两阶段在线实验（第II阶段参与者人数为491人；第I阶段提供了目标）来解决这一问题，参与者可以使用自己的部分资金减少那些之前在有或无大语言模型支持的情况下完成实际努力任务的同伴们的收益。
### Innovation
本研究通过实验证实了大语言模型用户的效率提升是以社会制裁为代价的，这一发现首次提供了行为证据。具体而言，研究揭示了大语言模型使用程度和惩罚之间的关系，以及披露大语言模型使用情况所导致的信誉差距。
### Conclusion
这些发现表明，大语言模型带来的效率增益是以社会制裁为代价的。当实际使用大语言模型的参与者被惩罚时，自我报告未使用大语言模型的参与者的惩罚更为严厉，表明对“未使用”的声明持怀疑态度。另一方面，在高使用水平下，实际依赖大语言模型的参与者比自我报告依赖大语言模型的参与者遭受更严格的惩罚，这一起表明了社会对大语言模型使用的复杂态度。
## 4. `cs.AI` - 大型语言模型基础对话代理拟人化的伦理视角综述 [PDF](https://arxiv.org/pdf/2601.09869), [HTML](https://arxiv.org/abs/2601.09869)
### Authors
Andrea Ferrario,Rasita Vinay,Matteo Casserini,Alessandro Facchini
### Background
随着大型语言模型（LLM）为基础的对话代理（CAs）日益增多，拟人化现象，即赋予非人类实体人类特征，变得越来越明显。与早期的聊天机器人相比，基于LLM的CAs生成交互和语言提示（如第一人称自我引用、观点和情感表达）更为频繁，这可以增加用户的参与度。然而，拟人化引发了如欺骗、过度依赖和剥削性关系构建等伦理问题，同时也有人认为拟人化交互可能支持自主性、幸福感和包容性。尽管在这一现象方面的兴趣不断增加，但现有文献在多个领域仍呈现碎片化状态，并且在定义、操作和规范性评估拟人化方面差异显著。
### Innovation
本文进行了一项涉及五个数据库和三个预印本仓储的范围审查，综合总结了关于基于LLM的CAs拟人化的伦理视角的概念基础、伦理挑战和机会、以及方法论方法。研究发现，在概念定义上存在共识，但在操作化、规范性框架以及将观察到的交互影响与可操作的治理指南进行关联方面存在显著差异。
### Conclusion
我们提出了一个研究议程，并为在LLM基础对话代理中伦理地使用拟人化提示提供了设计和治理建议。重点在于制定可操作的治理指南，以应对拟人化所带来的伦理挑战。
## 5. `cs.AI` - 在人机交互中，知识论为互补性提供未来 [PDF](https://arxiv.org/pdf/2601.09871), [HTML](https://arxiv.org/abs/2601.09871)
### Authors
Andrea Ferrario,Alessandro Facchini,Juan M. Durán
### Background
人的智能与人工智能系统的互补性是指在决策过程中，有人工智能系统支持的人类能够超越各自单独的能力。自从这一概念在人机交互领域提出以来，它通过推广依赖范型和提供一个更实用的替代‘对人工智能的信任’概念而获得了支持。然而，互补性面临一些关键性的理论挑战：缺乏精确的理论支撑，仅仅作为一种后验的预测准确性的评估指标，忽视了人类与人工智能互动的其他期望标准，并且忽略了其性能收益的规模成本属性。因此，在实际研究中很难实现互补性。
### Innovation
本文利用知识论来解决这些问题，通过在证成人工智能的讨论中重新定义互补性来应对这些挑战。文章借鉴计算可靠主义，认为互补性历史上的实例作为证据，证明了特定的人机交互是特定预测任务的可靠的认知过程。与其他可靠性指标一起，互补性有助于提高人机团队在生成预测时的可靠性。这将支持受影响者的实际推理，如患者、管理人员、监管者以及其他相关人员。
### Conclusion
我们的方法强调了互补性的角色和价值不在于提供预测准确性的相对度量，而在于帮助校准基于人工智能支持过程的可靠性来进行决策。
## 6. `cs.AI` - AI生存故事：AI末日风险的一种分类分析 [PDF](https://arxiv.org/pdf/2601.09765), [HTML](https://arxiv.org/abs/2601.09765)
### Authors
Herman Cappelen,Simon Goldstein,John Hawthorne
### Background
自从ChatGPT发布以来，社会各界对AI系统是否会对人类构成生存威胁展开了广泛讨论。本文提出了一种普遍框架来探讨AI系统可能对人类构成的末日风险。文章分析了一种基于两个前提观点的论点：AI将变得极为强大，而如果AI变得极其强大，将导致人类毁灭。
### Innovation
文章通过构建一个涵盖各种‘生存故事’的分类体系，分析了人类如何能在未来生存下来。每个‘生存故事’都说明一个前提无法实现，从而避免了人类被AI毁灭的局面。文中还指出每一类‘生存故事’面临的挑战不同，并对应不同的应对策略。
### Conclusion
通过这个分类体系，文章给出了人类毁灭由AI导致的概率（P(doom)）的大致估算。
## 7. `cs.AI` - 长时程大语言模型代理的连续记忆架构 [PDF](https://arxiv.org/pdf/2601.09913), [HTML](https://arxiv.org/abs/2601.09913)
### Authors
Joe Logan
### Background
检索增强生成（RAG）已经成为为大规模语言模型（LLM）代理提供背景知识的默认策略。然而，RAG 将记忆视为无状态的查找表：信息可以无限期地存储，检索是只读的，并且缺乏时间连续性。
### Innovation
作者定义了一种称为连续记忆架构（CMA）的新类系统。CMA 通过持久存储、有选择地保留、关联路由、时间连贯性以及将高阶抽象化纳入内部状态进行更新和维护。CMA 要求具有特定的架构规范，并在暴露 RAG 的结构限制（无法积累、修改或消歧义记忆）的任务中显示了一致的行为优势。实验证明了 CMA 是长周期代理所必需的架构原始元素，同时突出了关于延迟、漂移和可解释性的开放挑战。
### Conclusion
连续记忆架构是构建能够进行长时间任务的大语言模型代理的必要架构基本单元，但也提出了关于延迟、漂移和可解释性的研究成果中仍然存在的问题。
## 8. `cs.AI` - 通过注意力感知干预提高逻辑推理的链式思考 [PDF](https://arxiv.org/pdf/2601.09805), [HTML](https://arxiv.org/abs/2601.09805)
### Authors
Nguyen Minh Phuong,Dang Huu Tien,Naoya Inoue
### Background
现代使用大语言模型（LLMs）的逻辑推理主要依赖于复杂的交互式框架，将推理过程分解为通过精心设计的提示或借助外部资源（例如符号求解器）解决的子任务。尽管交互式方法增加了额外的开销，而混合方法则依赖外部组件，限制了其可扩展性。一种非交互式、端到端的框架可以让推理在模型内部产生，从而改善泛化能力，同时保持可分析性，无需任何外部资源。
### Innovation
本文介绍了一种非交互式、端到端的推理任务框架。通过在少样本提示中引入结构信息，激活了与逻辑推理操作模式一致的注意力头。基于这一见解，提出了注意力感知干预（AAI）方法，在推理时通过重新加权选定的逻辑模式对应注意力头的评分来引导模型的推理。AAI为通过注意力调制引导模型使用先验知识提供了一种高效的方式。
### Conclusion
广泛的实验表明，AAI在多个基准和模型架构上增强了逻辑推理性能，同时几乎不增加额外的计算开销。代码可访问此链接：this https URL。
## 9. `cs.AI` - 思考长远，但高效：大规模推理模型的稳定序列测试时序缩放 [PDF](https://arxiv.org/pdf/2601.09855), [HTML](https://arxiv.org/abs/2601.09855)
### Authors
Michael R. Metel,Yufei Cui,Boxing Chen,Prasanna Parthasarathi
### Background
序列测试时序缩放是一种无需训练的方法来提升大型推理模型的准确性，但目前这种方法存在显著限制。延长模型的推理长度可以提高其准确性，但推理长度的进一步延长会导致准确性下降和模型不稳定。
### Innovation
本文提出了一种新颖的序列测试时序缩放方法——Min-Seek，它在广泛诱导的思考范围内显著提高了模型准确性，稳定了序列缩放的准确性，并消除了推理长度微调的需求。该方法具有固有的高效性，仅在推理时保留一个额外诱导思想的KV对。通过自定义存储不带位置嵌入的键的KV缓存并动态连续编码每个新生成的思想，该方法可以在模型的最大上下文长度之外继续推理，并在轻微条件下具有线性计算复杂度。
### Conclusion
该方法不仅在多种推理任务中提高了模型准确性，而且是高效的，因为它只需要在推理时额外存储一个诱导思想的KV对。此外，通过动态编码方法，该方法能有效扩展推理长度，并保证高效率。
## 10. `cs.AI` - GUI-Eyes: 工具增强的感知在GUI代理中的视觉定位 [PDF](https://arxiv.org/pdf/2601.09770), [HTML](https://arxiv.org/abs/2601.09770)
### Authors
Chen Chen,Jiawei Shao,Dakuan Lu,Haoyi Hu,Xiangcheng Liu,Hantao Yao,Wu Liu
### Background
近期视觉语言模型（VLMs）和强化学习（RL）的进步促进了GUI自动化的发展。然而，当前大多数方法依赖于静态的一次性视觉输入和被动感知，缺乏适应性地决定何时、是否以及如何观测界面的能力。
### Innovation
本文提出了GUI-Eyes，一个用于GUI任务中主动视觉感知的强化学习框架。GUI-Eyes通过两阶段推理过程和适时的视觉工具（如裁剪或缩放）决定性策略，来实现更信息丰富的观测。同时，此框架引入了一种渐进感知策略，将决策分解为粗略探索和精细定位，通过双层策略进行协调。此外，还设计了一种空间连续的奖励函数，针对工具使用进行了定制，结合位置接近度和区域重叠来提供密集监督，解决GUI环境中普遍存在的奖励稀疏问题。
### Conclusion
在ScreenSpot-Pro基准测试中，GUI-Eyes-3B仅使用3k标注样本，实现了44.8%的定位准确率，显著优于监督和基于RL的基线。实验结果强调了分阶段策略推理和精细奖励反馈对构建稳健和数据高效GUI代理的重要性。
## 11. `cs.CL` - MT是否准备好应对下一次危机或大流行？ [PDF](https://arxiv.org/pdf/2601.10082), [HTML](https://arxiv.org/abs/2601.10082)
### Authors
Vipasha Bansal,Elizabeth Brown,Chelsea Kendrick,Benjamin Pong,William D. Lewis
### Background
在危机时刻，有效的沟通至关重要。然而，政府、援助提供者、医生和受这些人员帮助的人之间往往存在语言差异。在这种背景下，商业机器翻译系统成为可行的翻译工具。但这些系统在低资源语言（特别是在危机或医疗领域）中的翻译效果如何？为了评估这一点，研究者使用了包含与高优先级语言相关的疫情相关句子的数据集TICO-19，这些语言是由很有可能在未来疫情中受到负面影响的社区所使用的。
### Innovation
研究使用了TICO-19数据集来评估四款商业机器翻译系统在危机或医疗领域的翻译能力。这为评估商业机器翻译系统在低资源语言的危机应对中的适用性和准备状态提供了新的方法。
### Conclusion
研究基于输出翻译的易用性评估了商业机器翻译系统的当前“就绪”程度，以应对下一次疫情或紧急情况。
## 12. `cs.CL` - EHRNavigator：跨异质电子健康记录的患者级临床问题解答的多智能体系统 [PDF](https://arxiv.org/pdf/2601.10020), [HTML](https://arxiv.org/abs/2601.10020)
### Authors
Lingfei Qian,Mauro Giuffre,Yan Wang,Huan He,Qianqian Xie,Xuguang Ai,Xeuqing Peng,Fan Ma,Ruey-Ling Weng,Donald Wright,Adan Wang,Qingyu Chen,Vipina K. Keloth,Hua Xu
### Background
临床决策越来越依赖于及时获取和利用电子健康记录（EHRs）中的患者信息，但大多数现有的自然语言问题回答（QA）系统仅在基准数据集上进行评估，这限制了它们的实际应用价值。
### Innovation
本文引入了EHRNavigator，这是一个多代理框架，利用AI代理跨异质和多模态EHR数据进行患者级别的问题回答。通过使用公共基准数据集和机构数据集进行评估，展示了EHRNavigator在实际医院环境中，能够有效应对多样的数据模式、时间推理需求和多模态证据整合。
### Conclusion
通过定量评估和临床医生验证的病历复查，EHRNavigator在真实世界病例中的准确率达到86%，同时保持了临床可接受的响应时间。这些发现证实EHRNavigator有效地弥合了基准评估与临床部署之间的差距，提供了一种稳健、适应性强且高效的解决方案，用于真实的EHR问题回答。
## 13. `cs.CL` - 长链推理蒸馏通过自适应前缀对齐 [PDF](https://arxiv.org/pdf/2601.10064), [HTML](https://arxiv.org/abs/2601.10064)
### Authors
Zhenghao Liu,Zhuoyang Wu,Xinze Li,Yukun Yan,Shuo Wang,Zulong Chen,Yu Gu,Ge Yu,Maosong Sun
### Background
大型语言模型（LLMs）在解决复杂数学问题方面展示了显著的推理能力。最近的研究表明，通过蒸馏较长的推理路径，可以有效提高小型学生模型的推理性能。然而，教师生成的推理路径往往过长且结构复杂，难以被学生模型学习。这种不匹配导致提供的监督信号与学生模型的学习能力之间存在差距。
### Innovation
本文提出了一种名为Prefix-ALIGNment distillation（P-ALIGN）的框架，该框架通过自适应前缀对齐充分利用教师生成的中间推理（CoTs）进行蒸馏。P-ALIGN通过确定剩余后缀是否简洁且足以引导学生模型来适应性截断教师生成的推理路径。然后，P-ALIGN利用教师生成的前缀监督学生模型，促使有效的前缀对齐。
### Conclusion
在多个数学推理基准测试上的实验表明，P-ALIGN相较于所有基线方法有超过3%的性能提升。进一步分析表明，P-ALIGN生成的前缀提供了更有效的监督信号，同时避免了冗余和不确定推理成分的负面影响。所有代码可在该链接获取：this https URL。
## 14. `cs.CL` - ToolSafe：通过主动步骤级护栏和反馈增强LLM基代理的工具调用安全性 [PDF](https://arxiv.org/pdf/2601.10156), [HTML](https://arxiv.org/abs/2601.10156)
### Authors
Yutao Mou,Zhangchi Xue,Lijun Li,Peiyang Liu,Shikun Zhang,Wei Ye,Jing Shao
### Background
基于LLM的代理可以通过调用外部工具与环境互动，但其功能的扩展也放大了安全风险。实时监控代理在执行前的步骤级工具调用行为并主动干预至关重要，但这一领域仍处于探索阶段。
### Innovation
本工作首先构建了TS-Bench，一种新型的步骤级工具调用安全性检测基准。然后使用多任务强化学习开发了TS-Guard模型，该模型在执行前通过交互历史进行推理主动检测不安全的工具调用行为。它评估请求的危害性和动作-攻击相关性，产生可解释和可泛化的安全性判断和反馈。此外，引入了TS-Flow，一种基于护栏和反馈的推理框架，该框架在注入式提示攻击下将ReAct风格的代理的危害性工具调用减少了平均65%，并提升了约10%的良性任务完成率。
### Conclusion
该方法显著降低了不安全的工具调用行为，并在受到提示注入攻击时提高了良性任务的完成率。
## 15. `cs.CL` - 技能感知的数据选择与微调以实现高效推理蒸馏 [PDF](https://arxiv.org/pdf/2601.10109), [HTML](https://arxiv.org/abs/2601.10109)
### Authors
Lechen Zhang,Yunxiang Zhang,Wei Hu,Lu Wang
### Background
大型推理模型如DeepSeek-R1及其精简版本在复杂推理任务上表现优异，但也常常需要大规模数据进行监督微调。这推动了对数据高效训练方法的探索。
### Innovation
提出了一种以技能为中心的蒸馏框架，该框架通过两项内容实现了技能的有效转移：1）基于技能的数据选择，优先选择针对学生模型较弱技能的示例；2）技能感知的微调，在问题求解过程中鼓励明确的技能分解。
### Conclusion
仅使用100K教师生成语料中选出的1,000个训练样本，该方法在Qwen3-4B和Qwen3-8B上跨五个数学推理基准测试中的表现超过了随机监督微调基线1.6%和1.4%，进一步分析表明这些增益集中在训练中强调的技能上，突显了以技能为中心的训练方法在高效推理蒸馏中的有效性。
## 16. `cs.CL` - 从故事情节中提炼角色逻辑为编码决策树 [PDF](https://arxiv.org/pdf/2601.10080), [HTML](https://arxiv.org/abs/2601.10080)
### Authors
Letian Peng,Kun Zhou,Longfei Yun,Yupeng Hou,Jingbo Shang
### Background
现有的行为档案大多未结构化、不可执行且验证不足，导致代理人的行为脆弱。本文研究了角色扮演（RP）代理在不同叙述背景下行为保持一致的依赖行为档案的问题。
### Innovation
提出了编码决策树（CDT），这是一种数据驱动框架，能够从大规模叙事数据中推导出可执行且可解释的决策结构。CDT将行为档案表示为条件规则的决策树，内部节点对应于验证过的场景条件，叶子节点编码地块的行为陈述，这使得在执行时可以确定性检索上下文合适的规则。通过迭代地诱导候选场景-动作规则，用数据验证并通过对层级专业化进行精炼，CDT生成了支持透明检查和原则性更新的行为档案。
### Conclusion
在多个基准测试中，CDT在16个不同艺术作品中的85个角色上表现优于人工编写的档案和以往的行为档案生成方法，表明编码和验证的行为表示可以导致更可靠代理的基础。
## 17. `cs.CL` - CALM-IT：跟踪双重角色对话动力学生成现实的长时间动机访谈对话 [PDF](https://arxiv.org/pdf/2601.10085), [HTML](https://arxiv.org/abs/2601.10085)
### Authors
Viet Cuong Nguyen,Nhi Yen Nguyen,Kristin A. Candan,Mary Conlon,Vanessa Rumie,Kristen Risola,Srijan Kumar,Munmun De Choudhury
### Background
大语言模型（LLMs）在心理健康相关场景中越来越普遍，但它们在长时间互动中难以维持现实且目标导向的对话。尽管LLMs能够生成流畅的回答，但它们通常仅优化即将到来的对话回合，而不是保持与治疗进展相关的连贯模型，因此表现出脆性和远期偏差。
### Innovation
本文引入了CALM-IT框架，用于生成和评估长时间动机访谈（MI）对话，该框架明确建模了双重角色的对话动态。CALM-IT将治疗师-客户互动表示为双向状态空间过程，在这个过程中，两个代理不断更新相互作用和内心状态的推断实现策略选择和话语生成。
### Conclusion
CALM-IT在大规模评估中表现出色，其效果和目标一致性优于强大的基线模型，并且其稳定性随着对话长度的增加而保持相对稳定。虽然CALM-IT较少发起治疗师重定向，但它获得了最高的客户接受率（64.3%），表明干预时机更加精确和具有治疗性连贯性。总体而言，CALM-IT提供了建模不断演化的对话状态对于生成高质量的合成对话至关重要的证据。
## 18. `cs.CL` - SIN-Bench: 在长篇多模态科学交错文献中追踪原生证据链 [PDF](https://arxiv.org/pdf/2601.10108), [HTML](https://arxiv.org/abs/2601.10108)
### Authors
Yiming Ren,Junjie Wang,Yuxin Meng,Yihang Shi,Zhiqiang Lin,Ruihang Chu,Yiran Xu,Ziming Li,Yunfei Zhao,Zihan Wang,Yu Qiao,Ruiming Tang,Minghao Liu,Yujiu Yang
### Background
评估多模态大型语言模型是否真正理解长篇科学论文仍然具有挑战性。当前用于评估的方法，如仅基于答案的指标和合成的“针扎干草堆”测试，往往奖励答案匹配，而没有要求模型提供因果关系和证据链的推理过程。因此，亟需提出一种方法来要求模型在原生科学文献中构造明示的跨模态证据链。
### Innovation
该研究提出了名为‘海洋中的鱼’(FITO)的新范式，要求模型在原始科学文献中构建明确的跨模态证据链。为此，作者构建了SIN-Data科学交织语料库，保持了文本与图表的原生交织，并在此基础上构建了SIN-Bench，包含四个逐步任务：证据发现、假设验证、基于事实的问答和证据锚定的综合。此外，引入了“没有证据，就没有分数”的评分机制，将预测与可验证的锚点对接，并通过匹配度、相关性和逻辑诊断证据质量。
### Conclusion
实验结果显示，对齐证据是主要瓶颈。Gemini-3-pro在总体得分上表现最佳（0.573），而GPT-5的SIN-QA问题回答准确性最高（0.767），但在对齐证据的综合评分中表现不佳，这表明正确性和可追溯支持之间存在差距。
## 19. `cs.CL` - EmplifAI：28种情感标签下精细日本同理心医疗对话数据集 [PDF](https://arxiv.org/pdf/2601.10033), [HTML](https://arxiv.org/abs/2601.10033)
### Authors
Wan Jou She,Lis Kanashiro Pereira,Fei Cheng,Sakiko Yahata,Panote Siriaraya,Eiji Aramaki
### Background
慢性疾病患者在疾病管理的不同阶段会经历各种情感波动，从希望到绝望。现有数据集在支持这些患者方面存在不足，没有提供具体的情感导向对话场景。EmplifAI通过引入28种细粒度情感类别的情感导向对话场景，旨在解决这些问题。
### Innovation
EmplifAI是一个专为日本设计的同理心对话数据集，涵盖了28个具体的情感类别。该数据集包含了280个医学背景情况下的对话场景。通过使用BERTScore评估情感一致性，研究比较了基础日语语言模型和 fine-tuned 后的模型在流畅性、通用同情心和特定情感同情心方面的表现，并通过人类评分和LLM评分进行了比较，以验证评估管道的有效性。
### Conclusion
EmplifAI通过为不同情感阶段的慢性疾病患者提供特定场景下的对话指导，有效提高了模型的情感理解和生成能力。这种方法为医学对话系统的发展提供了新的见解和验证方法，同时也揭示了使用AI系统潜在风险的相关讨论。
## 20. `cs.CL` - 由大规模语言模型驱动的角色扮演代理：当前状况、挑战与未来趋势 [PDF](https://arxiv.org/pdf/2601.10122), [HTML](https://arxiv.org/abs/2601.10122)
### Authors
Ye Wang,Jiaxing Chen,Hongjiang Xiao
### Background
近年来，随着大型语言模型（LLMs）的迅速发展，角色扮演语言代理（RPLAs）在自然语言处理（NLP）和人机交互领域的交叉点上成为研究重点。该论文系统地回顾了RPLAs的当前发展和关键技术，从早期基于规则的模板方法、语言风格模仿阶段，到以个性建模和记忆机制为中心的认知模拟阶段。论文还总结了支持高质量角色扮演的关键技术路径，包括心理量表驱动的角色建模、记忆增强指令机制以及动机和情境为基础的行为决策控制。
### Innovation
该论文详细梳理了RPLAs的技术演进，包括从规则基础模板方法到语言风格模仿再到认知模拟（以个性建模和记忆机制为中心）的技术发展历程。此外，它还分析了构建角色特定语料库的方法和挑战，概述了评价框架和基准数据集，涵盖了角色知识、个性一致性、价值观对齐和互动幻觉等方面，并讨论了人类评估、奖励模型和基于LLM的评分方法的优点和缺点。
### Conclusion
该论文提出了角色扮演代理未来的发展方向，包括个性演化建模、多代理协作叙事、多模态沉浸式互动和与认知神经科学的整合，旨在为后续研究提供系统的视角和方法论上的借鉴。
